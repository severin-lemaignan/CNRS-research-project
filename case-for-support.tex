
%\chapter{Research project}\label{part1}

\section{Long-term vision and ground-breaking nature of the project}

\begin{framed}

\noindent\bf This research project is about designing and
delivering a ground-breaking embodied AI for socially intelligent robots, with
long-term social utility and demonstrated acceptance in the real world.

\vspace{0.4em}
\noindent This breakthrough is made possible by a combination of novel methodologies and
the principled integration of complex socio-cognitive capabilities:

\begin{itemize}
        \item crowd-sourced social interaction patterns;
        \item `public-in-the-loop' machine learning;
        \item a novel spatio-temporal and social model of the robot's environment;
        \item novel, non-repetitive, social behaviour production based on
            generative neural networks;
        \item and finally, an integrative cognitive architecture, driven by
            long-term social goals.
\end{itemize}


\vspace{0.4em}
\noindent In addition, I will deliver the conceptual and ethical framework
required to further support the public debate and policy making process
around social robots, and concretely demonstrate lifescale applications of
this technology with ambitious, long-term deployments of autonomous robots
in high impact, social environments.

%% TODO LAAS
%\vspace{0.4em}
%\noindent The Laboratoire d'Analyse et d'Architecture des Syst√®mes (LAAS), part
%of the Artificial and Natural Intelligence Toulouse Institute (ANITI), would
%be an ideal host laboratory to successfully conduct this programme: its
%strong track-record in autonomous interactive robots, combined with the breadth
%of expertise available within the ANITI institute, would prove instrumental in
%scaffolding and accelerating several of the key science breakthrough I target
%with this project.

\vspace{0.4em}
\noindent Closely aligned with the national and European research priorities,
this research project creates a excellent opportunity to assert the CNRS and
Europe as worldwide leaders in Social and Intelligent Robotics.

\end{framed}

\subsection{State of the art: real-world social robots and impact on the
society}

Social robotics is a disruptive field, with a profound impact on society and
economy~\parencite{williams2020social}. A recent report from the United Nations about
the impact of the technological revolution on labour markets stated that AI and
robotics are expected to radically change the labor market world-wide destroying
some job categories and creating others~\parencite{bruckner2017frontier}. Social
robotics, however, is still an young, emerging, research-active field. The
expectations are high, in multiple application domains: elderly care, customer
service (in airports and shopping malls, for instance), education, child
development, and autonomous vehicles to name a few~\parencite{baillie2019challenges}.
However, whereas both computer-based AI applications, and traditional industrial
robots already have a significant economic impact, social robots have not
reached that point yet. Significantly, the recent failures of several companies
investing in social robotics, like Jibo, Kuri, Willow Garage and Anki, and the
major setbacks of companies like SoftBank, who designed and deployed hundreds of
Pepper robot in their shops, before renouncing a few months later due to the poor
reception by the customers, show that these technologies are not yet
mature~\parencite{tulli2019great}.

Indeed, understanding \emph{why} these robots have failed, is one of the
active debate within the Human-Robot Interaction
community~\parencite{hoffman2019anki}, with only a handful of qualitative studies on
this question~\parencite{dereshev2019longterm,degraaf2017phased}. Proposed
explanations include the lack of perceived usefulness (robot seen purely as a
toy); the limited liveliness of the robot that become rapidly predictable and
repetitive~\parencite{lemaignan2014cognitive}; the poor management of expectations,
where user over-ascribe cognitive capabilities that do not match the reality.
The community agrees however that the crux of the issue is achieving long-term
social engagement~\parencite{yang2018grand,hoffman2019anki}

Research is however seemingly hitting a wall to further progress towards
socially meaningful long-term interactions. For instance, in their large review
of research in robotics for education, \textcite{belpaeme2018social}
point to the shortcomings that prevent further development of effective,
long-term social robotics in educative settings: the need for a correct
interpretation of the social environment; the difficulty of action selection;
the difficulty of pacing generated behaviours: three issues that underpin
long-term engagement.

Attempts at long-term human-robot interactions are nevertheless becoming more
common~\parencite{kunze2018artificial,leite2013social}, with a number of studies
involving social robots deployed in real-world settings (for instance in
schools~\parencite{leite2014empathic,westlund2017measuring,
lemaignan2016learning,coninx2016towards}, homes~\parencite{degraaf2017phased} and
care centres~\parencite{hawes2017strands,winkle2020couch}) over relatively long
periods of time (up to 2 or 3 months at a time). Even though these robots are
typically not fully autonomous, they do exhibit a level of autonomy, either by
handling autonomously a relatively broad range of shallow tasks (eg, a
butler-like robot answering simple questions, like in~\textcite{hawes2017strands} or
in the H2020 MuMMer~\parencite{heikkila2018can} and FP7
Spencer project~\parencite{triebel2016spencer}), or a narrow, well-specified complex
task (for instance, supporting exercising in a gym, as I did in~\textcite{winkle2020couch}).
However, general purpose, long-term interaction is still an open question.

\subsection{Framing and objectives}

The overall aim of my research programme is to \textbf{create, sustain and better
understand the dynamics of responsible, long-term social human-robot
interactions}. This translates into three overarching, long-term research questions:

\vspace{0.5em}
\begin{itemize}
    \item What are the public expectations with respect to the role
        of social robots, and how can we \textbf{collectively design} principles ensuring
        \textbf{autonomous}, yet \textbf{responsible, beneficial, socially acceptable robots}?

    \item What are the conceptual, algorithmic and technical
        prerequisites to design and implement such an autonomous \& responsible
        robots? in particular, what
        AI is required to \textbf{sustain long-term engagement} between
        end-users and a robot?

    \item What new ethical questions are raised by long-term social
        interaction with an artificial agent, and in particular, how to balance
        \textbf{autonomy} of the robot with \textbf{behaviour transparency} and
        \textbf{human oversight}?
\end{itemize}

\vspace{0.5em}
\noindent From these questions, I derive the following five objectives that are
the guiding principles of my research programme:

\paragraph{\bf O1: conceptual framing} To construct a solid conceptual framing
around the multidisciplinary question of responsible human-robot interactions,
answering questions like: What should motivate the robot to step in
and attempt to help? or: What social norms are applicable to the robot behaviours? I
will investigate the basic principles of responsible social interactions, that
must form the foundations of a socially useful robot, accepted and used in the
long run.  Using user-centred design and participatory design methodologies, I
will identify the determinants and parameters of a responsible social
intervention, performed by a socially-driven robot, and formalise them in
practical principles.

\paragraph{\bf O2: real-time social modeling} Critical to long-term sustain
engagement is the understanding by the robot of its social environment: this
objective is to create the novel cognitive
capability of artificial \emph{social situation assessment} and enable the
robot to represent real-time social dynamics in its environment, I will
significantly extend and integrate the current state-of-art in social signal
processing and spatio-temporal modeling (so-called \emph{situation assessment})
with my recent research in social state modeling, exploring as well novel
avenues like \emph{social embeddings} to enable better machine learning.

\paragraph{\bf O3: congruent social behaviours production} 
I want to explore and create novel ways of producing non-repetitive,
socially-congruent, expressive behaviours. This includes for instance expressive
social motions using state-of-the-art generative neural networks to co-invent
with artists (eg choreographers) a novel `body language' for robots.

\paragraph{\bf O4: embodied AI breakthrough}
I aim to create robot behaviours that are perceived as purposeful and
intentional (long-term goals), while being shaped by a user-created and
user-controlled action policy. I will integrate long-term social goals, arising
from the interaction principles of \textbf{O1}, with the social modeling
capability of \textbf{O2} and the behaviours production of \textbf{O3} into a
principled, goal-driven cognitive architecture. The breakthrough will come from
combining these long-term social goals with bottom-up action policies, designed
and learnt from the end-users using human-in-the-loop reinforcement learning.

I want to specifically test the following two hypotheses: first, that long-term
social goals, if suitably co-designed with the public and stakeholders and
properly integrated into the robot as a \emph{social teleology}, will create the
perception that the robot is intentional and purposeful. This will in turn
elicit sustained engagement from its human users.

Second, that human-in-the-loop machine learning can
be used to ensure an additional layer of human oversight and a level of
behavioural transparency.  Human-in-the-loop reinforcement learning -- as
implemented in the SPARC approach that I have developed with my students and
already used in complex social
environments~\parencite{senft2017supervised,senft2019teaching,winkle2020insitu}
-- relies on an end-user `teacher'. This teacher initially fully controls the
robot (via teleoperation) while it learns the action policy, and then
progressively relinquishes control up to a point where the robot is effectively
autonomous. As I previsouly argued in~\textcite{senft2019teaching}, this approach
leads to increased control and ownership of the system, and as a result,
increased trust from the end-users.

This objective also raises one additional question: how to \emph{arbitrate}
between a top-down action policy arising from the long-term goals and the
bottom-up action policy learnt from the end-users? This question leads to
objective {\bf O4'}: To design a policy arbitration mechanism that preserves the
robot's long-term intentional behaviour while effectively guaranteeing human
control, ownership and oversight.

\paragraph{\bf O5: ambitious field research} Finally, the last major objective
of my research project is to demonstrate the effectiveness of my approach in
complex, real-world conditions. This means deploying the socially interactive
robots in existing social eco-systems that are sufficiently complex and open to
explore novel social interactions. My objective is also to show that this
real-world deployment can be successfully driven by the `end-to-end' involvement
of all the end-users and stakeholders: from defining the robot's role, from the
different perspective of each end-user, to actually designing and `teaching' the
robot what to do.

\begin{framed}

\noindent\bf Together, these five objectives build a coherent and realistic
pathway towards addressing the overall aim of my research programme:
creating, sustaining and better understanding the dynamics of responsible
long-term social human-robot interactions.

\end{framed}



%Two paradigms form the scientific backbone of the project: (1) for end-users to
%ascribe social utility and engage with the robot over long periods of time
%(months, years), the robot has to have its own long-term internal motivation to
%be socially helpful -- what we call a \emph{social teleology}; (2) long-term
%acceptance requires the genuine involvement of end-users at every step of the
%design process, so that they take \emph{ownership} of the technology --
%human-in-the-loop machine learning offers a promissing way forward, by
%permitting to design robot behaviours that genuinely originate from the
%end-users themselves.



%How this general
%principle translates into specific guidelines and algorithms -- while taking into
%account the principles of a responsible AI -- is the central
%contribution of Work Package 1.

% This socially-driven goal forms what we call a \emph{social
%teleology}. its own goals have this objective can only be achieved if the
%robot is \textbf{socially-driven}: the robot's behaviours must be driven by the
%intention to support positive human-human interactions. 


%I frame these principles within the broader concept of \textbf{robot-supported human-human
%interactions}, a novel conceptual framework to make it possible to `think' the future human-robot
%interactions at the societal level. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{3em}
\section{Research programme}


These five scientific objectives underpin my research vision and scientific
programme. This section presents how I intend to implement these objectives,
i.e.  what are the major research directions that I will research, develop and
establish as active research fields in the coming years.

\vspace{0.5em}
I intend to organise my research along \textbf{4+1 main research
strands}:

\vspace{0.5em}
\begin{itemize}
    \item {\bf Strand 1} focuses on advancing the \textbf{perception of complex social
        situations}, including modeling the complexity of humans and human group
        dynamics (objectives O1, O2);
    \item {\bf Strand 2} investigates the \textbf{intelligent generation of social
        behaviours}, exploring novel techniques mixing immersive teleoperation
        and adversarial generative networks (objective O3);
    \item {\bf Strand 3} aims at significantly progressing the state-of-art in
        \textbf{cognitive architectures} for robots, also accounting for and integrating
        end-users in the generation of cognitive behaviours (objectives O1, O4).
    \item {\bf Strand 4} focuses on framing and practically advancing what
        responsible and safe AI means in the context of social robots.
        Critically, I propose a methodology enabling the co-construction of
        these guidelines with both the general public and ethics expert. This
        work will pave the way for an international framework and concrete
        guidelines for \textbf{responsible
        human-robot interactions} (objective O1).
\end{itemize}
\vspace{0.5em}


Those four research strands are all underpinned by one additional research
activity (objective O5), realised in the transversal {\bf Strand 5}.
% TODO LAAS
%As a
%research scientist, I will build-up an ambitious \textbf{experimental capacity}
%at LAAS-CNRS, that will significantly improve upon the current, mostly
%lab-based, experimental work conducted to date.
Building on my own extensive
experience in real-world deployments of autonomous social robots (as outlined in
my academic track-record), I will establish an ambitious experimental programme,
in close partnership with local institutions, based on the field's best
practices that I have contributed to
establish~\parencite{baxter2016characterising}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Overview and coherence of the research programme}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{wps}
\caption{Overview of the research strands that I intend to develop as a CNRS
    research scientist.}
\label{fig:wps}
\end{figure}

These five strands are tightly coupled, and together will enable a major
scientific and technical breakthrough in autonomous, socially-intelligent,
robots.

Figure~\ref{fig:wps} gives an overview of how the research directions interact
with each other. Fieldwork (\textbf{Strand 5}) plays a central role in my research programme, and
appears in the centre of the figure. Indeed, the deployment of autonomous social
robots in real-world, meaningful social spaces (eg. in schools, hospitals,
museums and science centres, etc) will be
integral to my research methodology, and will enable the development of
`public-in-the-loop' experiments (5.1): the public, by co-designing interventions,
interacting and, at time, taking direct control of the robots, will shape what a
useful and socially acceptable interaction looks like, and lead to the
\emph{definition of core interaction principles}. Using machine learning to learn from
these field experiments (3.2), these core principles are in turn translated into
algorithmic models, guiding the \emph{social teleology} of the cognitive architecture
(3.1).

The regular fieldwork I intend to conduct will also provide the source of data
to feed into Strand 1: \textbf{Strand 1}, focusing on \emph{social situation
assessment}, researches, develops, and integrates all the components pertaining
to the assessment of the spatio-temporal and social environment of the robot.
Reference interaction situations and the interaction datasets required to
support this research is directly drawn from the experimental fieldwork, as well
as an additional, focused experimental programme on mental states modeling that
I detail hereafter.

These perceptual capabilities are both continuously integrated into the robot's
cognitive architecture (3.3), iteratively improving the socio-cognitive
performances of the robot, and disseminated to the broader community through
standardisation and integration to the international Robot Operating System
(ROS) ecosystem.

\textbf{Strand 2} looks into behaviour generation using immersive teleoperation
to investigate novel non-verbal interaction modalities (2.1), combined with
new developments in machine learning to learn and automatically generate them
(2.2). In this research strand, I will focus on researching new way of
automatically generating rich behaviours (including eg expressive gestures,
expressive motions) that are non-repetitive and socially congruent. I intend to
apply state-of-the-art deep generative networks to achieve this; as such, the
research strand is data-intensive, and will use datasets acquired during the
field deployments, as well as lab-recorded dataset of social interactions, using
novel immersive techniques presented below. Similar to Strand 1, the newly
developed capability of generating socially congruent behaviours is continuously
integrated in the robot architecture.

%\subsubsection*{Towards building a principled cognitive architecture}

Finally, an important part of my research programme contributes to the design
and implementation of a novel, principled cognitive architecture for socially
intelligent robots (\textbf{Strand 3}). In addition
to the integration of Strand 1 and Strand 2, Strand 3 is also about
researching and developing the socio-cognitive principles, or \emph{drives}, of
the architecture. They will be identified both from the 'public-in-the-loop'
research and end-user engagement conducted in \textbf{Strand 4}, and an novel
research on intrinsic social motivation that I detail below.


The following sections describe in greater detail each of these research
strands, in the context of the current state-of-the-art.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Strand 1: \textbf{\wpTwo}}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.9\linewidth]{strand1}
    \caption{Social situation assessment, from feature extraction to 3D
    modelling, to social \emph{embeddings} and symbolic reasoning. I will focus
    on using ROS for implementation, facilitating broad and lasting impact on
    the community.}
    \label{fig:strand1}
\end{figure}

My first research direction will look into integrating a full representation
system for the social environment of the robot. It builds on existing state of
art in \emph{situation assessment} and \emph{knowledge representation}, and
extend it to the social sphere (Figure~\ref{fig:strand1}).

Indeed, knowledge representation and grounding is a fundamental building block
for cognitive architectures, as I have previously
shown~\parencite{lemaignan2017artificial}. In fact this first research strand
builds on existing work on symbolic knowledge representation
(eg.~\parencite{tenorth2009knowrob, lemaignan2010oro})
and my work on situation assessment~\parencite{lemaignan2018underworlds,
sallami2019simulation} to create a coherent system of representations for the
cognitive architecture, that I will further improve with recent advances in symbolic
(eg.  data-driven semantic labelling, like the 4D convolution network
MinkowskiNet~\parencite{choy20194d}) and hybrid (like
\emph{conceptors}~\parencite{jaeger2014controlling}) representations
capabilities.

The main contribution of the research strand, however, is on \textbf{augmenting
this traditional spatio-temporal environment modeling with social
representations}. I intend to work on three specific aspects: (1) modeling humans
and social dynamics; (2) integrating these models into the cognitive perception
of robots; (3) specifying and computing \emph{social embeddings}, a
low-dimensional synthetic representation of the social surroundings of the
robots.

\subsubsection{1.1 -- Multi-modal human model; interaction and group dynamics}

This first research activity focuses on the acquisition, processing and modelling of social
signals~\parencite{gunes2017automatic} to build a multi-modal model of the humans
in the robot's vicinity. I have recently introduced a dataset of social
interaction~\parencite{lemaignan2018pinsoro} that enables for the first time a
quantitative, data-driven investigation of social dynamics. Promising initial
results led me to uncover three latent constructs that underpin social
interactions~\parencite{bartlett2019what}. I want to combine this emerging
data-driven paradigm with my previous research on the psycho-social determinants
of the interaction (anthropomorphism, trust, persuasion, etc.) to define and
implement a holistic, multi-modal model of the human, suitable for real-time
HRI.

From modelling the human, I will then investigate automatic understanding and
modelling of group-level social interactions~\parencite{tapus2019perceiving},
including $f$-formations~\parencite{marshall2011using}, sociograms (as done
in~\textcite{garcia2016hybrid} for instance), and inter-personal
affordances~\parencite{pandey2013affordance}. This task builds on literature on
social dynamics analysis, eg~\parencite{durantin2017social,jermann2009physical,
martinez2019collocated},
and apply it to real-time social assessment by a robot, itself embedded into the
interaction.

\begin{outcome}{1.1}
    A holistic model of humans, suitable for modelling human-robot and
    human-human interactions with high granularity; an algorithmic pipeline for
    the automatic analysis of social dynamics at group-level, able to model in
    real-time the social context of the robot.
\end{outcome}


\subsubsection{1.2 -- Social situation assessment}


In 1.2, I integrate the social cues from 1.1 with traditional situation
assessment platforms. It will result in a socio-cognitive model of the social
environment of the robot that I term \emph{social situation assessment}. It
effectively extends the existing spatio-temporal representation capabilities of
robots to the social sphere, and covers the development of a complete social
assessment pipeline, from social signal perception (like automatic attention
tracking, face recognition, sound localisation, etc.) to higher-level
socio-cognitive constructs, including group dynamics, perspective
taking~\parencite{flavell1992perspectives} and mutual
modeling/mentalizing~\autocite{lemaignan2015mutual, dillenbourg2016symmetry}.

\begin{outcome}{1.2}
A novel cognitive sub-system for social situation assessment, integrated in an
open-source software framework (see technical contributions
page~\pageref{sec:technical}). This cognitive system will enable the robot
to represent its physical and social environment, and perform queries about
it, including queries about past events (temporal model) and queries
requiring higher socio-cognitive perceptual capabilities like perspective taking
and mentalizing.
\end{outcome}


\subsubsection{1.3 -- Social embeddings}

One of the key novel scientific idea that I will research in this strand is
the construction of a meaningful \textbf{social embedding} for the robot.
\emph{Embeddings}, as a machine learning concept, are compact, low-dimensional
representations of much more complex phenomena. A well-known example of such embeddings
is \emph{word embeddings} (eg.
\emph{word2vec}~\autocite{mikolov2013distributed} or \emph{GloVe}~\autocite{pennington2014glove}) where
words are projected into a $n$-dimensional space (with eg. $n = 300$ in the case
of \emph{GloVe}). Such an embedding can be constructed such as Euclidian
distance in the embedding reflects semantic distance in the word-space, making
it computationally inexpensive to represent and manipulate words' semantics.
Similar embeddings are used in social signal processing, for instance for
face~\autocite{schroff2015facenet} or gesture~\autocite{ge2008hand} recognition.

I believe a similar approach could be devised for social interactions:
constructing a low-dimensional embedding onto which social situations can be
projected while they are perceived by the robot.
Note that the additional complexity introduced by the dynamic nature of the interaction
(ie, a social situation needs to be observed for some time to be correctly
interpreted) could in principle be overcome using for instance a transformer
architecture~\autocite{vaswani2017attention}, as done for action recognition
in eg.~\textcite{girdhar2019video}.


If fruitful, this approach would significantly simplify the application of
neural networks to automatically recognise social situations and social dynamics
(something notoriously difficult to achieve with the current
state-of-art, as I discuss in~\textcite{bartlett2019what}), and potentially \emph{generate}
plausible social situations, that the robot could use to eg. predict the next
states of an interaction.

\begin{outcome}{1.3}
The investigation of \emph{social
embeddings} as a general, sub-symbolic representation of the social
environment of interactive robots. This would unlock the widespread use of
machine learning techniques to investigate social AI.
\end{outcome}

\subsubsection{Focused experimental programme}

In complement to the large-scale experimental work described in Strand 5, a
focused experimental programme accompanies Strand 1, to specifically support and
demonstrate the investigation of these socio-cognitive capabilities.

I intend implement a subset of the experimental protocols identified
by~\textcite{frith1994autism} to investigate theory of mind with autistic
children, as it offers an excellent experimental framework for social robotics,
as I argued in~\textcite{lemaignan2015mutual}.  Indeed, experimental protocols
in research on autistic spectrum disorders are often striking by their apparent
straightforwardness because of the careful choice of interaction modalities:
since autistic children frequently exhibit impairments beyond social ones (such
as motor or linguistic ones), the experiments must be designed such that they
require only basic cognitive skills beyond the social abilities that are tested.
This makes them especially well-suited for experimental robotics, properly
isolating the cognitive competencies that we want to test and evidence early on
in the development process of a complete cognitive architecture.

\begin{table}[h]
    \centering
    \begin{tabular}{p{0.4\linewidth}p{0.5\linewidth}}
        \toprule
        No mentalizing required           & Mentalizing required          \\
        \midrule
        Ordering behavioural pictures     & Ordering mentalistic pictures~\parencite{baron1986mechanical} \\
        Understanding see                 & Understanding know~\parencite{perner1989exploration}            \\
        Protoimperative pointing          & Protodeclarative pointing~\parencite{baron1989perceptual}     \\
        Sabotage                          & Deception~\parencite{sodian1992deception}                     \\
        False photographs                 & False beliefs~\parencite{leslie1992domain}                 \\
        Recognizing happiness and sadness & Recognizing surprise~\parencite{baron1993children}          \\
        Object occlusion                  & Information occlusion~\parencite{baron1992out}         \\
        Literal expression                & Metaphorical expression~\parencite{happe1993communicative}       \\
        \bottomrule
    \end{tabular}
    \caption{\small Tasks requiring or not mentalizing to pass, listed by~\textcite{frith1994autism}}
    \label{mentalizing-tasks}
\end{table}

Frith and Happ√©'s list (Table~\ref{mentalizing-tasks}) is in that regard
especially interesting in that it mirrors pairs of task (ones which do not
require mentalizing with similar ones which do require mentalizing), thus
providing good control tasks.
I intend to implement several of these tasks to support Strand 1's contributions
to basic research in artificial social cognition --
\emph{Object occlusion} vs.~\emph{Information
occlusion}~\parencite{baron1992out}, for instance, evidences \emph{representation-level} perspective taking: adapting such a
protocol for human-robot pairs would demonstrate \emph{second-order},
\emph{representation-level} perspective taking capabilities, which is beyond the
state-of-the-art in an artificial cognitive system.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Strand 2: \textbf{\wpThree}} 


Mirroring Strand 1's focus on understanding the social interactions, Strand 2
addresses the question of social behaviour \emph{generation}: how to create
natural behaviours, engaging over a sustained period of time (eg not simply
picking scripted behaviours from a library, that are rapidly perceived as
repetitive).

The focus of my research will be in the first instance on \emph{non-verbal}
behaviours. This is a purposeful interaction design choice,
that ensures we can more effectively manage what cognitive capabilities are
ascribed to the robot by the users (expectation management).  I seek however to
significantly push forward the state-of-the-art of behaviour generation for
robots, both in term of technique to generate the behaviours, and in term of the
nature of the non-verbal behaviours (including expressive gestures and motion,
non-verbal utterances using sounds, gaze, joint attention).

% TODO LAAS
%This strand of research will build upon the long expertise of the LAAS-CNRS on
%developing social robots interacting with humans in complex
%environments~\autocite{Lallement2014,gharbi2013natural,
%waldhart2015planning}\TODO{add additional/better references}, as well as the
%existing literature on current behaviour generation methodologies, covering
%techniques like curiosity-driven behaviours~\parencite{oudeyer2005playground},
%Learning from Demonstration~\autocite{billard2008robot, argall2009survey},
%human-in-the-loop action policy learning~\autocite{senft2016sparc,
%senft2019teaching}.
%
%\begin{collaboration}{Nicholas Asher, IRIT, ANITI}
%Dr. Nicholas Asher is a leading figure in natural language processing.
%Collaborating with him would open strong mutual opportunities to develop and
%apply language processing for physical, embodied agents.
%the LAAS would enable me to 
%\end{collaboration}


\subsubsection{2.1 -- Design of novel interaction modalities}

%\begin{figure}[!htbp]
%\centering
%    \includegraphics[width=0.7\textwidth]{figs/cozmo-expression-sheet.jpg}
%\caption{Cozmo facial expressions}
%\end{figure}

As part of Strand 2, I intend to lead research on novel non-verbal
interaction modality for social robots. I will pursue an interdisciplinary,
bottom-up approach involving artists and field practitioners, exploring new
communication modalities like
creating a body language for social robot with choreographers, or
investigating new forms of sound expressions like soundscapes with sound
experts (soundscapes are sound environments that reflects a
particular situation; they have been shown to be an effective intervention
technique in the context special needs treatments, eg~\textcite{greher2010soundscape}).

I specifically want to question the traditionally uncreative approach of HRI
where human codes of communication are typically replicated `as it' on robots.
To discover new modes of interaction, potentially better-suited to sustain
long-term engagement, I want to invite artists and creators of diverse
backgrounds to create new, original communication codes (like I did with
comedians in the \emph{roboscopie} theatre
play~\autocite{lemaignan2012roboscopie}).

As an illustration, Figure~\ref{fig:puppet-robot} shows an immersive setup where
a dancer or a choreographer `takes control' of the robot's body via motion
capture, while seeing through the robots' eyes (VR headset or a CAVE projection,
similar to~\parencite{bailly2015beaming}).  For a period of time, the artist
`wizard' is tasked to invent a body language for the robot while interacting
with the humans it meets, by remotely controlling the robot. This would result
in a large dataset of freely created, socially-congruent physical behaviours, that would directly
feed into eg Generative Adversarial Networks, as presented below.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/puppet.pdf}
    \caption{\label{fig:puppet-robot} (left) Possible appearance of a
    puppet-robot that I will use to collect data. A tablet, displaying facial
    animations, is mounted on a robotic arm.  It can freely orient its `gaze'
    and use expressive movements. The robot is effectively teleoperated in
    realtime by an artist (eg choreographer, right) who 'sees through the
    robots' eyes' and who is tasked with designing and
    acting a novel 'body language' for social interactions.}

\end{figure}

%TODO LAAS
%\begin{collaboration}{Olivier Stasse, LAAS-CNRS}
%Olivier Stasse is an expert in humanoid motion, and has past experience in
%working with dancers to inform the design of complex motions. I have a
%track-record of collaboration with him on art-driven design
%methodologies~\parencite{lemaignan2012roboscopie}.
%\end{collaboration}

\begin{outcome}{2.1}
The research, development and
    implementation of novel non-verbal communication modalities, including for
    instance a robot 'body language' for social interactions and soundscapes; a
    large dataset of such interactions, recorded in immersive conditions, and
    suitable for machine learning.

\end{outcome}


\subsubsection{2.2 -- Generative neural network for social behaviour production}

%Designing behaviours that
%enable sustained, long-term engagement in a social human-robot interaction is
%essentially an open research question. Three main approaches to social
%behaviours generation exist today: \emph{user-induced}, where the end-user
%interacts with the robot and ascribes (knowingly or not) complex behaviours to
%the machine, while in reality the robot's behaviours are simple and non-goal
%oriented (eg generating a noise or a small movement when being touched). This
%has been used to great effect in therapy robots, for instance (eg Paro).
%\emph{Off-the-shelf behaviours}, where the robot relies on a set library of
%behaviours (that might be individually relatively complex). The approach can
%elicit a strong initial social response from the user, but this social response
%tends to vanish rapidly once the `tricks' of the robot have been all discovered
%and become repetitive.  Besides, as the robot does not typically maintain a
%long-term socio-cognitive plan of the interaction, the behaviours are typically
%perceived as fun, yet pointless. This is often observed in toy-like robots (eg
%Vector, Dot \& Dash). Finally, many social robots avoid altogether the problem
%of generating behaviours by simply offering to the end-user control over
%\emph{low-level behaviours} (eg, control of the joints of the robot). This means
%that, even when the robot has relatively powerful social perception capabilities
%(like recognising people and voice), no real social behaviours is generated.
%
%None of these three approaches are satisfactory, and indeed, no approach to date
%has been able to engage human users in long-term, sustained
%interactions~\parencite{hoffman2019anki}.

Designing behaviours that enable sustained, long-term engagement in a social
human-robot interaction is essentially an open research question. The specific
challenge of producing non-repetitive social behaviours is particularly
difficult: social robots typically rely on off-the-shelf behaviours,
where the robot effectively picks from a set library of pre-programmed
behaviours. The approach can elicit a strong initial social response from the
user, but this social response tends to vanish rapidly once the `tricks' of the
robot have been all discovered and become
repetitive~\autocite{lemaignan2014cognitive}.  Besides, as the robot
does not typically maintain a long-term socio-cognitive plan of the interaction,
the behaviours are typically perceived as fun, yet pointless, leading to
disengagement. This is often observed in toy-like robots (eg Vector, Dot \&
Dash)~\parencite{hoffman2019anki}.

The CNRS has played a pioneering role in this field with eg the development of
HATP as a hybrid task-planner for human-robot
interaction~\cite{Alili2008,Lallement2014} or human-aware motion
planning~\cite{gharbi2013natural, waldhart2015planning}.  While effectively
enabling the robot to store and manage long-term plans, symbolic task planners
still rely on mostly static libraries of `canned', repetitive actions. Also
neither of these planners are well-suited to rapid, dynamic behaviours
generation, especially in situations requiring performing parallel, blended
actions.

Building nevertheless on these solid foundations, I aim at significantly
advancing the state of the art in this regard, by combining two recent
machine-learning techniques: (1) generative neural networks for affective robot
motion
generation~\parencite{yang2019appgan,marmpena2019generating,suguitan2020moveae};
(2) interactive machine learning in high-dimensional input/output spaces, where
I have shown with my students promising results for generating complex social
behaviours~\parencite{senft2019teaching, winkle2020insitu} that fully involve
the end-users~\parencite{winkle2018social}.

Specifically in~\textcite{suguitan2020moveae}, a Generative Adversarial Network
(GAN) is trained to generate expressive motions, modulated
by a feature encoding an emotion. I will extend this idea in two ways: (1) I
will train the GAN on multiple interaction modalities (motions, but also facial
expressions, gaze, sounds) using the data acquired in 2.1, effectively creating
a new multi-modal `grammar' for the robot expression.  (2) Instead of using
emotions to modulate the generation stage, I will use the social embedding
constructed in 1.3: the generated behaviours will be shaped by the current,
complex social state of the interaction instead of simply emotions.

%At a time where companion robots are coming to the market, one important
%question remains fully open: how to design robot behaviours that foster
%lasting engagement? A vast body of academic literature identifies that
%robots evoke an initial phase of high user engagement (the
%\emph{novelty} phase) that vanishes as the user realises that the robot
%is actually quite predictable and repetitive. The \emph{agency}
%initially ascribed by the user to the robot quickly
%fades\parencite{lemaignan2014dynamics}, leading to critical user disengagement from the technology.
%
%
%The (often limited) library of behaviours available to the robot is
%often cited as a key factor in causing this issue. However, another,
%more profound issue affecting long term engagement with robot companions
%is the question of \emph{purpose.} Without clear \emph{purpose}, social
%robot companions can lack \emph{usefulness}. Indeed, robot
%\emph{companions} might not have explicit goals that would dictate or
%motivate their behaviours: they aim at providing a social presence, a
%social comfort, as cats or dogs would do, without necessarily being
%goal-oriented.
%
%Recent attempts -- and failures -- to convert social robotics research
%into commercial platforms (Jibo, Kuri and most recently Anki's Cozmo and
%Vector robots) reflect exactly this, with reasons for their failure
%typically citing an under-delivery of the user experience they promised,
%and/or the lack of a `real need' to justify their price point. The
%\project project addresses these two key issues by:
%
%\begin{enumerate}
%\def\labelenumi{\arabic{enumi}.}
%\item
%  Taking inspiration from human-pet relationships which also have no
%  explicit \emph{purpose} beyond their potential for enjoyable,
%  \emph{affective} interactions;
%\item
%  Working with creative professionals who excel at storytelling and
%  emotional engagement to overcome the problems in sustaining
%        engagement, as proposed by Hoffman\parencite{hoffman2019anki}
%\item
%  Blending these two sources of inspiration using a radically novel
%  combination of immersive teleoperation and machine learning.
%\end{enumerate}
%
%The project is \emph{not} about replicating a pet's behaviour per se. It
%is instead about identifying, modeling and automatically generating the
%social behaviours required to recreate pet-like social dynamics between
%robots and humans, drawing inspiration from ethology (Stanton, Sullivan,
%and Fazio 2015). Using animal behaviours to inform the design of robots
%is not new, the most remarkable example being the Sony AIBO robot dog,
%whose behaviours were directly designed around those of actual dogs
%(Arkin et al. 2003). However, to go beyond the repetitive interactions
%associated with such robots, we propose to employ a creative
%professional to actively participate in design and automation of \project
%behaviour. The concept of using creative professionals to `teach' social
%robot behaviour is not new either (Knight and Gray 2012), however it is
%only recent advances in human-in-the-loop, online machine learning that
%make this type of real-time `social training' a feasible approach to
%generating and automating engaging social behaviours (Senft et al.
%2019).
%
%Our project has the following goals, addressed by the workplan presented
%below:
%
%\begin{enumerate}
%\def\labelenumi{\arabic{enumi}.}
%\item
%  assemble a non-anthropomorphic social robot that can autonomously
%  navigate in a complex and living lab environment, taking inspiration
%  from ethology to inspire the robot's behaviour;
%\item
%  develop an immersive teleoperation system, enabling a creative
%  professional to `take control' of the robot (i.e.~puppet the robot) in
%  a completely intuitive way (using whole body motion tracking);
%\item
%  record (and make publicly available) a large dataset of social
%  behaviours (created through immersive teleoperation) that foster
%  long-term social and affective engagement. The dataset will also
%  include the social \emph{signals} implicitly used by the puppeteer to
%  drive his/her choice of actions (recorded through eg eye-tracking);
%\item
%  using machine learning, map these social signals (input state) to the
%  robot behaviours (output state) such that the robot can operate
%  autonomously.
%\end{enumerate}
%
%A creative professional (puppeteer, dancer or comedian --
%corresponding financial compensation is budgeted) will join the group.
%First she/he will take part to a one-week co-design workshop (4) aiming
%at finalising the immersive teleoperation controller and the behaviours
%of the robot. Then, she/he will interact for about 4 hours a day during
%a month, with the BRL lab members (200+ researchers). She/he will do so
%by remotely operating the robot (5) from an (out-of-sight) control room
%(the BRL CAVE room). The aim will be for the puppeteer to pro-actively
%engage with people in the lab, attempting to engage in \emph{social,
%affective} interactions. This will be achieved by creating/inventing
%in-situ a new `grammar' of social behaviour, loosely inspired by those
%of cats and other pets. These interactions will be fully recorded
%(including eye-tracking on the puppeeter) (6), in order to create a
%unique dataset of complex social interactions, suitable for machine
%learning. The PI has already extensive experience in recording such
%datasets (see (Lemaignan et al. 2018) for instance).
%
%%%\begin{wrapfigure}[17]{l}{8cm}
%%\begin{figure}
%%    \centering
%%    \includegraphics[width=0.8\paperwidth]{figs/dev.pdf}
%%    \caption{\label{fig:support}
%%    Social behaviours will be learned from immersive `puppetering' of the
%%    robot, performed by a professional actor. The `puppetering' takes place
%%    in a CAVE (or VR) environment, where what the robot `sees' and `hears'
%%    is streamed live}
%%\end{figure}
%%%\end{wrapfigure}
%
%Over the following four months, a deep neural network will be designed
%and trained (7) for the regression task of generating continuous social
%behaviours from perceived social signals. In parallel, a software
%controller will be developed (8) to enable generic autonomous
%capabilities (like autonomous navigation) for which the BRL has
%extensive expertise.
%
%Finally, the last four months will be dedicated to in-situ testing of
%the autonomous system (9). We will seek to conduct a large scale study
%within the lab, over a period of several weeks. For this study, the
%robot is expected to be fully autonomous. However sufficient amount of
%time is planned for additional iterations on the development of the
%robot controller if deemed necessary. We aim at publishing the results
%of this main study shortly after the end of the one-year period.

\begin{outcome}{2.2}
An algorithmic approach inspired by generative neural networks to produce
non-verbal yet multi-modal social behaviours. They will combine expressive
gestures, gazing behaviours, facial expressions, and expressive sounds.
\end{outcome}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Strand 3: \textbf{\wpFour}}




Strand 3 investigates the principled integration of a cognitive
architecture for autonomous social robots. It binds together the socio-cognitive
perceptual capabilities of the robot developed in the Strand 1, the action
production mechanisms developed in the Strand 2, and includes key elements from
my 'human-in-the-loop' methodology to isolate and model the interaction principles
and social goals of the robot (Figure~\ref{fig:archi}).

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figs/archi}
\caption{Contributions of my research strands to the AI architecture at the core
    of the research programme.}
\label{fig:archi}
\end{figure}

Strand 3 integrates the robot cognitive capabilities in a new cognitive
architecture for long-term social autonomy. It introduces a novel arbitration
mechanism between action policies, to enable both long-term, goal-driven
autonomous behaviours (conveying \emph{intentionality}, an important factor of
human-robot engagement as shown by~\textcite{wiese2017robots}), and direct
in-situ learning from the robot's end-users, to ensure transparency and human
oversight.




\subsubsection{3.1 -- A social teleology for robots}

\emph{Teleological systems} (ie goal-driven) have been investigated in robotics
for being a way of providing long-term drives to an autonomous robot. This has
been successfully applied to relatively simple cognitive
systems~\parencite{oudeyer2005playground,moulinfrier2014self} or virtual
agents~\parencite{pathak2017curiosity}. This first basic research activity in
Strand 3 aims to significantly progress this line of research, and to look into
complex interactive cognitive systems. The key objective of this activity
is to define and implement a novel \emph{social teleology}: the
algorithmic encoding of long-term social goals into the robot.

This work will directly draw from the participatory, `human-in-the-loop',
methodological paradigms that I present in Strand 5. Indeed, before being
transposed into algorithms, these long-term social goals will first be
co-defined and co-created by the end-users and the public in terms of
\emph{interaction principles} for useful and responsible social robots.

\begin{outcome}{3.1}
    The framing and algorithmic definition of a \emph{social teleology} for
    interactive robots: an intentional, long-term, socially-driven action policy
    for human-robot interactions.
\end{outcome}

\subsubsection{3.2 -- Learning from humans to achieve by-design responsible \&
trustworthy AI}

I have recently obtained promising results on human-in-the-loop social
learning~\parencite[published in Science
Robotics]{senft2019teaching},\parencite{winkle2020insitu}: non-expert end-users
teach a robot in-situ (eg at school, at the gym, etc. and during the normal
operations of the setting), which progressively learns to be autonomous,
eventually reaching full task- and social autonomy.  This
reinforcement learning-based approach, that I
initially developed with one of my students~\parencite{senft2017supervised},
holds a lot of promise in term of field acceptance of social robots as it
entrusts the end-user with a high level of control during the learning phase,
leading to a feeling of ownership of the resulting robot behaviours.  I will
further develop this approach, with a special focus on real-world interaction
situations where a diverse set of robot skills might need to the learnt in
parallel.

In addition, I will study and evidence through qualitative methods whether (and how)
human-in-the-loop machine learning enables a more trustworthy and transparent AI
system, by involving the end-users in the creation of the robot behaviours.

\begin{outcome}{3.2}
A human-in-the-loop reinforcement
    learning paradigm, suitable for in-situ teaching of the robot by the
    end-users themselves, demonstrated in complex, multi-task, social environments.
\end{outcome}

\subsubsection{3.3 -- Integrating a socially-driven architecture for long-term interaction}

This research activity builds on my past research on cognitive
architectures~\autocite{baxter2016cognitive} and the current state of art in this
area (both disembodied
architectures~\parencite{chong2007integrated,vernon2007survey,kingdon2008review,duch2008cognitive,langley2009cognitive,taatgen2010past,thorisson2012cognitive}
and architectures for interactive robots -- ACT-R/E~\parencite{trafton2013act}, HAMMER~\parencite{demiris2006hierarchical},
PEIS Ecology~\parencite{saffiotti2005peis,daoutis2012cooperative},
CRAM/KnowRob~\parencite{beetz2010cram, tenorth2009knowrob},
KeJia~\parencite{chen2010developing}, POETICON++~\parencite{antunes2016from}, 
the LAAS Architecture for Social
Interaction~\parencite{lemaignan2017artificial}, to which I have been a key
contributor during my PhD).

The overall purpose of this new socio-cognitive architecture is to integrate in
a principled way the spatio-temporal and social knowledge of the robot (Strand
1) with a decision-making mechanism, to eventually produce socially-suitable
actions (Strand 3).  The decision-making mechanism is critical, and lay at the
heart of my research project. The robot will rely on it to generate action
decision that are purposeful, legible and engaging on the long run, something
that none of the existing architectures have been able to successfully
demonstrate to date. I aim at a breakthrough, and will introduce a novel
approach: drawing from the interaction patterns investigated in Strands 4 and 5,
I will combine long-term, socially-driven goals (the \emph{social teleology},
3.1), and human-in-the-loop machine learning (3.2) using a novel arbitration
mechanism.

The arbitration mechanism itself will build on research on reinforcement
learning for experience transfer~\parencite{madden2004transfer} that enables the
re-assessement of a policy (here, our long-term social teleology) based on
specific experience (here, the end-user-taught policy).


\begin{outcome}{3.3}
A socio-cognitive architecture, fully implemented
on interactive robotic platfoms, that enables long-term social engagement, by combining
long-term goals with domain-specific action policies, taught by the
end-users themselves.
\end{outcome}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Strand 4: \textbf{\wpOne}}

The basic, long-term ambition of my research programme is to revisit the
underpinnings of human-robot interaction by taking a \textbf{strong
human-centered perspective}. I frame this as a shift from \emph{human-robot
interaction} to \ul{\emph{robot-supported human-human interactions}} (r-HHI).
This major strand of research operationalises this objective in term of basic
contributions: on one hand, framing the interplay between r-HHI, responsible AI,
and ethics; on the other hand, driving this change of paradigm through my own
scientific and technical work, as well as within the broad HRI community. This
will be directly influenced by the experiemental work described in Strand 5.

\subsubsection{Conceptual framing of r-HHI and ethical framework} 

The first task in Strand 4 is to research and define the conceptual frame around
questions like: what role should social robots have?  Where to set the
boundaries of artificial social interactions? What does `ethical-by-design',
`responsible-by-design' mean in the context of social human-robot interactions?

Indeed, my research project involves social robots, interacting in repeated ways
and over long period of time, with human end-users, including vulnerable ones
(like children).  This raises complex ethical issues, both practical ones (how
to design the experimental work in a such a way that they are safe and ethically
sound), and more fundamental ones (what is the ethical framework for robots
intervening in socially sensitive environment?).

The ethical questions raised by social robotics have been actively studied over
the last 5 years, attempting to address issues like:

\begin{itemize}
    \item how to ensure that social robots are not used to simply replace the human
        workforce to cut costs?
    \item can we provide guarantees that the use of social robots will always be
        ethically motivated?
    \item further on, can we implement some ethical safeguarding built-in
        the system (like the ethical \emph{black-box} advocated by~\textcite{winfield2017case})?
    \item what about privacy? how to trust robots in our home or school or
        hospital not to eavesdrop on our private lives, and, in the worst
        case, not be used \emph{against} us?
\end{itemize}

These questions are indeed pressing. The recent rise of personal assistants like
Amazon Alexa or Google Home, with the major privacy concerns that followed
their deployments in people home, shows that letting the industry set the agenda
on these questions is not entirely wise -- and robots can potentially be much
more intrusive than non-mobile smart speakers.  The EU is positioning itself at
the forefront of those questions. The recent release of operational Ethics
Guidelines for Trustworthy AI by the EU High-level Expert Group on Artificial
Intelligence~\parencite{eu2019ethics} is a strong sign of this commitment. These
guidelines identify seven requirements for a trustworthy AI:

%\begin{enumerate}[label=\textbf{R\arabic*}]
\begin{itemize}
    \item \textbf{Human agency and oversight}, including
            fundamental rights, human agency and human oversight

    \item \textbf{Technical robustness and safety}, including resilience to
        attack and security, fall back plan and general safety, accuracy,
        reliability and reproducibility

    \item \textbf{Privacy and data governance}, including respect for privacy,
        quality and integrity of data, and access to data

    \item \textbf{Transparency}, including traceability, explainability and
        communication

    \item \textbf{Diversity, non-discrimination and fairness}, including the
        avoidance of unfair bias, accessibility and universal design, and
        stakeholder participation

    \item \textbf{Societal and environmental wellbeing}, including
        sustainability and environmental friendliness, social impact, society
        and democracy

    \item \textbf{Accountability}, including auditability, minimisation and
        reporting of negative impact, trade-offs and redress.

%\end{enumerate}
\end{itemize}

The design methodologies and techniques employed in my research programme
implement most of these requirements: interaction co-design and
human-in-the-loop machine learning ensures human agency oversight over the
robot's behaviours; the transparency of the robot behaviour stems from the
machine learning approach that I advocate (the robot's behaviours primarily
originate from what the end-users themselves taught the robot); diversity and
non-discrimination is supported by the large-scale involvement of the public at
my experimental programme, ensuring a broad diversity of backgrounds and
profiles; societal wellbeing is the core research question of the project, and I
intend to contribute in realising this requirement in the context of social
robots.  Technical robustness and accountability are important additional
requirements, that will guide my technical work.


Indeed, the Ethics Guidelines for Trustworthy AI form a solid foundation for the
project. However, personal and social robots raise additional questions
regarding what ethical and trustworthy systems might look like, and while the
principles of responsible design are somewhat
established~\parencite{stahl2016ethics, bsi2016robots}, the consequences of eg
the social influence that robots can generate is not fully understood yet (as I
show in~\textcite{winkle2019effective}), if only because the technology required
to experience such interactions is only slowly maturing. 

Social robots have indeed two properties that stand out, and distinguish them
from smart speakers, for instance.  First, they are fully embodied, and they
physically interact with their environment, from moving around, to picking up
objects, to looking at you; second, willingly or not, they are ascribed much
stronger \emph{agency} by people. This second difference has far-reaching
consequences, from affective bonding to over-trust, to over-disclosure of
personal, possibly sensitive,
informations~\parencite{martelaro2016tell,shiomi2017robot}.  As an example, a
common objection to human-robot interaction is the perceived deceptive nature of
the robot's role. It has been argued~\parencite{biscontilucidi2018companion}
that the underlying concern is likely the lack of an adequate (and novel) model
of human-robot interactions to refer to, to which the project will provide
elements of response. This needs nevertheless to be accounted for in depth.

Ethical framing of social robotics has started to
emerge under the term \textbf{roboethics}: the ``subfield of applied ethics
studying both the positive and negative implications of robotics for individuals
and society, with a view to inspire the moral design, development and use of
so-called intelligent/autonomous robots, and help prevent their misuse against
humankind.''~\parencite{allen2011robot}. Specific subfields, like assistive
robotics~\parencite{sharkey2012granny}, have seen some additional work, but social
robotics is still not equipped with operational guidelines, similar to the EU
guidelines on trustworthy AI.

I want my research to significantly contribute to the framing and the building
of guidelines and recommendations for responsible human-robot interactions,
enabling a safe and trustworthy  digital future in our society.  I intend to
develop this line of research by engaging with national and international policy
makers (for instance, I am already acting as an Expert Collaborator on ethics
for the European Joint Research Centre), but also by adopting the same
open-science approach, involving the general public in the process: my field
work experiments (Strand 5) will both \emph{build on} and \emph{feed into} the
framework developed in this research strand.

\begin{outcome}{4}
A conceptual framework that clarify and organise together the questions raised
by long-term social interactions; ethical guidelines for such
interactions, aimed at informing future policy making.

\end{outcome}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% WeTheCurious
% 
% - one robot completelty controlled by children, one by adults
% 
% what to learn?
% 
% - when to approach? when to prompt? [example of the salesman/museum facilitator]
% - when is the right time to help/intervene or not? 'child being told off by
% parents -> not the right time!'
% - group interactions -> when to intervene? what about peer-pressure? eg what if
% I tell off one child in front of another?
% - break the barrier for participation. Japanese Journal paper -> facilitating students questions
% - impact on moral norms? what behaviours is acceptable?
% - what role for the robot? another mediator? a peer?
% - what can we do with that 'alien creature'
% 
% - robot taking one child to talk to the museum mediators ("I, robot, am  shy!
% would you come with me?")
% 
% - learning how to adjust behaviour based on personality
% - 'why do I behave like that with that person, and like this with that other
% person?'
% 
% - reinforcement learning instead of human-in-the-loop -> what reinforcement
% signal? engagement
% 
% - the robot that 'take sides': take side against the adults? -> bending in its
% role?
% 
% 
% - social embarassment
% - space for pretence: the robot can adopt an 'artificial role' as long as it is
% possible (accpetable/...) to pretend the robot is



\subsection{Strand 5: \textbf{\wpFive}}

I intend to scaffold and demonstrate the results of my research with an
ambitious experimental programme. This experimental programme has three main
purposes: (1) informing the design of desirable social robots by creating a
'crowd-scale' methodology to co-design and co-construct the robots' social role
and behaviours with the end-users and general public; (2) acquiring large
dataset of complex, real-world interactions that will feed back into my
research; (3) demonstrating meaningful progress on robot-supported social
interventions, in complex and high-impact real-world environments.



\subsubsection{5.1 -- Crowd-sourced patterns of robot-supported social
interactions}

In order to broadly engage the public with defining what future robots should do
to be perceived as responsible, beneficial, and engaging, I intend to create and
deploy a novel investigation methodology that I term `experimental
crowd-sourcing'. 
For an extensive period of time (six months to a year), and in close partnership with local institutions, I will
deploy social robots inside of public spaces (eg museums), relinquishing their control
to the visitors themselves. Tasked with remotely operating the robot to assist
fellow visitors, the participants will `invent by doing' a new
grammar of social interactions, and sketch out answers to questions like ``what does it
mean for a robot to help?'', ``How to do so in the dynamic, messy, public
environments?'', ``What are acceptable behaviours?'', ``Can we see new social norms
emerge?'' At the end of this experiment, we expect 1000s of people to have
experienced -- and co-designed -- how robots should interact with humans in a
positive, helpful way. Each of these experiences will contribute to uncovering
and designing the basic principles of social interaction for robots.

From a technical perspective, this work will be based on my previous research on
`human-in-the-loop' machine
learning~\autocite{senft2019teaching,winkle2020insitu}, scaled up both
in term of duration (several months) and volume of input (hundreds of
participants).

% TODO LAAS
%\begin{collaboration}{Toulouse Science Centre \emph{Le Quai des Savoirs}
%    \newline \& local science charities}
%
%    This work will only be successful if it reaches out to a broad and diverse
%    public. I have an established network of contact in the local science
%    communication community (eg local science charities \emph{Plan√®te Sciences},
%    \emph{Les Petits D√©brouillards}) and intend to establish a solid
%    collaboration with Toulouse Science Centre \emph{Le Quai des Savoirs} where
%    field studies could be run.
%    
%\end{collaboration}

\begin{outcome}{5.1}
A large-scale engagement of the general public into `thinking' the roles of
robots, informing the work of Stranf 4; a set of crowd-sourced interaction
patterns and principles, that will help designing the long-term social goals of the
robot (3.1); a large dataset of social interactions to feed into Strands 1
and 2.
\end{outcome}

\subsubsection{5.2 -- Experimental fieldwork}

%TODO LAAS
%I intend to create a strong culture of Human-Robot Interaction experimental work
%at the LAAS-CNRS:
As mentioned in my track-record, I have 7+ years of experience
deploying robots `in the wild' (including schools, gyms, medical surgeries)
where robots are used by non-experts in ecologically valid environments. When
conducted with high standards of scientific rigour, these
deployments provide invaluable insights on the actual challenges that prevent
broader acceptance and use of robots in the society. In the first 5 to 7 years of
starting my role at CNRS, I will strive to demonstrate my approach with at least two
such ambitious field experiments.

The first one would typically involve the deployment of social robots in local
special needs schools (SEN schools), building on the initial pilots that I have
conducted at the Bristol Robotics Lab. Building on a rigorous participatory
approach involving the school teachers, as well as the parents, we would seek to
integrate robots in the daily life of the schools, supporting the development of
the students' physical and social skills.

The second one could for instance take place at a children's hospital, supporting
isolated children who suffer long-term conditions in close cooperation with the
hospital staff. Likewise, social robots would be deployed on premises, for at
least one uninterrupted year. They would integrate the daily routines of the
institution, under supervised autonomy~\parencite{senft2017supervised}, and
\emph{without} requiring or expecting the presence of a researcher at all time.

These two examples raise specific practical and ethical questions, as they
target vulnerable populations. This is however an informed choice: with their
complex social situations and social dynamics, they would allow to convincingly
demonstrate the importance and positive impact of socially-driven,
socially-responsible robotics.  When implemented, these scenarios would also
actually deliver high societal impact, with tenths of children to directly
benefit of the project. This would show how robots can have a lasting,
beneficial impact on the society, alongside human carers, and it will establish
the idea of \emph{robots supporting human interactions} instead of dehumanising
our social relationships.

Importantly, these deployments would take place within the strict ethical framework
established in Strand 4, providing real-world feedback on the suitability and
thoroughness of the ethical guidelines.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Developing methodological and technical excellence}
\label{sec:technical}

Finally, I would like to sketch out my intended contributions to one key
underpinning of my research programme: technical and methodological excellence.
Indeed, it is important to re-state here the fundamental nature of my project:
\textbf{the development of novel, fully autonomous, social robots, able to
foster long-term interactions}. This aim is beyond the state of the art, and
while ambitious science is required to achieve it, \textbf{technical excellence
is the fundamental enabler}. I am excited about such a major technical
challenge, and following on from my strong technical track-record, I will
innovate to build and assemble the leading technology required to bring
autonomous social robots to the general public.

\subsection{Advancing open-source software for HRI}

As outlined in the presentation of my past contributions, I have a long academic
track-record of technical open-source contributions to robotic algorithms and
software; to name a few significant ones: the \texttt{oro} knowledge
base~\autocite{lemaignan2010oro}, the natural language processing with semantic
grounding tool \texttt{dialogs} \autocite{lemaignan2011grounding}, the 3D
situation assessment platform
\texttt{underworlds}~\autocite{lemaignan2018underworlds}, the LAAS architecture
for social interaction~\autocite{lemaignan2017artificial}, or new algorithms
for interactive reinforcement learning~\autocite{senft2017supervised}). I intend
to pursue these efforts, also embracing and integrating novels techniques from
neighbouring fields. Indeed, astonishing progress has been made over the past
ten years in AI, largely due to the success of machine learning techniques to
classify complex signals. As highlighted in the previous sections, I intend to
fully embrace these techniques when and where relevant (for instance,
\emph{social embeddings} to recognise complex social situations or generative
networks to create on the fly complex social gestures).

One critical aspect of the technology that I want to investigate in
depth is \textbf{real-world robustness}: indeed, many of the impressive
achievements of machine learning do not hold well when applied to messy,
real-world situations. While \emph{not} an intrinsic weakness of machine
learning, it reflects a lack of focus and experience with the more challenging
experimental conditions encountered when robots are deployed outside of the
labs. As presented in Strand 5, I have an ambitious experimental programme,
which will provide complex real-world data to train next-generation deep neural
networks, suitable for robust social perception and action generation.

I will carry on this commitment to developing new software and algorithms with a
special attention to the software engineering challenges that are often
overlooked by academics and, as a result, hamper dissemination: for instance,
continuous integration, packaging, automated testing using simulation, deep
integration with existing frameworks and standards (especially ROS).
%TODO LAAS
%The
%LAAS-CNRS has a very strong tradition of software engineering (eg.
%GenoM~\autocite{mallet2010genom3}, \texttt{robotpkg}), and I hope to be able to
%build on and further develop this capacity.

The challenges raised by (the lack of) real-world robustness and software
complexity are especially visible when building social perception pipelines (the
main goal of my research strand 1). It effectively requires to
combine together tenths of specialist software components into one coherent and
principled framework, with algorithmic redundancy, and complex
testing requirements. I have initiated this work
(Figure~\ref{fig:ros4hri}), with preliminary steps in place
for multi-modal human modeling. However, providing real-world robustness and overcoming
training sets weaknesses and biases will be a long-term effort that I intend to
spearhead as a CNRS researcher.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{pipeline}
    \caption{Overview of the 'ROS4HRI' framework~\autocite{mohamed2020ros} that
    I designed and whose development I currently supervise.}

    \label{fig:ros4hri}
\end{figure}

\subsection{Future-proofing robotic platforms for interaction}

%\begin{figure}[ht]
\begin{wrapfigure}[16]{l}{0.5\linewidth}
    \centering
    \includegraphics[width=0.7\linewidth]{self-balancing-robots}
    \caption{\small \href{https://www.halodi.com/ever3}{HALODI Eve r3} (left)
    and IIT R1~\parencite{parmiggiani2017design} (right). These state-of-the-art
    robots are designed with compliant actuation, supporting safe interaction
    with humans.  Their agility make them well-suited for complex real-world
    environments.}

    \label{fig:selfbalancingrobots}
%\end{figure}
\end{wrapfigure}

%TODO LAAS
%The LAAS-CNRS, as a potentially host laboratory, has created and owns a solid
%set of mobile platforms for interaction: two WillowGarage PR2, SoftBank Peppers,
%bespoke dual manipulators\TODO{what else?}

My research programme focuses on the \ul{AI engine} of social robot, rather than
eg hardware development. As such, my work relies on pre-existing platforms,
suitable for social human-robot interactions.


However, looking specifically at human-sized mobile manipulators with advanced
social features, the choice of robotic platforms is in effect limited.  The two
leading mobile social robots available on the market today, SoftBank Pepper and
PAL TiaGo (along with the Fetch Mobile Manipulator, which is functionally
similar to PAL's TiaGo) are 5+ years old design. While they have played an
important role in supporting research in HRI over the past years, they do not
reflect the current state of the art in social mobile manipulators
%(see Table~\ref{robot-comparison})
, and as a CNRS researcher, I will seek internal
and/or external funding to acquire state-of-the-art platforms like the HALODI
Eve or the IIT R1 (Figure~\ref{fig:selfbalancingrobots}), eg via joint
applications to EU funding.


%\begin{table}[h!]
%    \caption{Comparison of HALODI Eve robot with PAL TiaGo and
%    Softbank Pepper. HALODI Eve (and similarly IIT R1) is a compliant mobile
%    dual manipulator with advanced social interaction capabilities.}
%
%    \resizebox{\linewidth}{!}{
%    \begin{tabular}{@{}p{3cm}p{5cm}p{5cm}p{5cm}@{}}
%\toprule
%                                           & PAL TiaGo                                                & Softbank Pepper                                                           & \textbf{HALODI Eve}                                                                                                                              \\ \midrule
%\textbf{Social features}                   & Poor (non-expressive head)                               & Expressive, yet fixed, face; limited gaze; approachable & Screen-based expressive face, incl. eyes; approachable \\
%\textbf{Perception}                        & Medium (RGB-D camera; laser scanner; no microphone)      & Medium (RGB-D; simple mic array; poor laser scanner)                      & Excellent (SotA RGB-D; 7-mic array; laser scanner)                                                                                                   \\
%\textbf{Navigation}                        & Good (however, limited agility due to large footprint)   & Poor (weak localisation capabilities)                                     & Good (high agility due to Segway-like self-balancing)                                                                                                   \\
%\textbf{Safety}                            & Medium (heavy robot; large footprint; non-compliant arm) & Medium (smaller footprint; safe arms; limited stability)                  & Good (small footprint; safe arms; dynamic stability)                                                                                       \\
%\textbf{Manipulation capabilities}         & Medium (non-anthropomorphic gripper; single arm)         & Limited (poor gripper with low payload; dual arm)                         & Good (anthropomorphic gripper; pressure sensors; dual arm)                                                                    \\
%\textbf{Suitability for human environments} & Poor (relatively large, difficult to clean)              & Good (smaller footprint, easy to clean)                                   & Good (small footprint, easy to clean)                                                                                                      \\ \bottomrule
%\end{tabular}
%}
%    \label{robot-comparison}
%\end{table}
%

\subsection{Fostering open-science best practices}

Finally, I see fostering \textbf{open science best practices} as a cornerstone
of technical and methodological excellence. I am a long-standing open science
proponent and I have led a number of targeted actions to further develop these
practices: the publication and an analysis of the current practices and
recommendations in field of human-robot
interaction~\autocite{baxter2016characterising}; the publication of large open
datasets~\autocite{childrenspeech2016, pinsoro2018, lemaignan2018pinsoro}; open
access publishing; a large corpus of open-source contributions
(\href{https://github.com/severin-lemaignan}{180+ public repositories on
Github}), including key contributions to major research software like ROS (I led
the port of ROS to Python3); active and open engagement with the wider community
via social media.

More can be done, however, and as a CNRS researcher, I would commit to further
enhance the 

I will leverage my editorial role in high-profile publications and
conferences (most notably FrontiersIn Robotics and AI, and the ACM/IEEE
Human-Robot Interaction conference) to advance this agenda.

\begin{itemize}
    \item \textbf{reproducible science} by always providing the code and data
        analysis scripts used in experiments;
    \item \textbf{pre-registered studies} where the exact methodology is
        published \emph{before} performing the study itself, ensuring the
        highest standard of experimental rigour;
    \item \textbf{continued lobbying} in international academic venues to
        support and foster open-science best practices, in particular by opening
        the Human-Robot Interaction conference to pre-registered studies
\end{itemize}

\TODO{finish section}


%\subsubsection{Experimental approach}
%
%My experimental approach has two phases. First, I will co-design and
%co-construct the robot's social role and behaviours through large-scale public
%engagement. For a whole year, I will deploy the \project robot within the Open
%City Lab of Bristol Science Centre \emph{WeTheCurious}, relinquishing its
%control to the visitors themselves. Tasked with remotely operating the robot to
%assist fellow visitors, a researcher will accompany them in `inventing by doing' a new
%grammar of social interactions to develop answers to the questions: what does it
%mean for a robot to help? How to do so in the dynamic, messy, environment of a
%science centre? What are acceptable behaviours? Can we see new social norms
%emerge? At the end of this experiment, we expect 1000s of people to have
%experienced -- and co-designed -- how robots should interact with humans in a
%positive, helpful way. Each of these experiences will contribute to
%uncovering and designing the basic principles of social interaction for robots.
%This work is the focus of WP1.
%
%\begin{wrapfigure}[11]{l}{0.15\linewidth}
%    \centering
%    \vspace{-10pt}
%    \includegraphics[width=\linewidth]{halodi-eve.jpg}
%    \label{fig|EVE}
%\end{wrapfigure}
%
%While most of the interactions in the science centre will be short-lived, a
%follow-on, long term (one year) experiment will take place in one of Bristol's
%Special Education Needs (SEN) school where I currently run pilots, helping 250+
%children with psycho-social impairments (autism) to develop their social skills
%and to engage into playful social activities: telling stories, triggering group
%activities with other children, providing additional social presence. Similar to
%the science museum experiment, the robot behaviours will be co-designed with,
%and learnt from the end-users themselves: teachers, parents, and as much as possible,
%the children themselves.
%
%
%Importantly, \project focuses specifically on the \ul{AI engine} of the robot: I
%will use an existing robotic platform (Halodi's EVE, pictured on the left) and
%develop and train the algorithms required to achieve autonomy and responsible,
%long-term social utility. Indeed, after an initial training period, the robot
%will be \ul{autonomous}: while the users will be provided with tools to override
%the robot's decisions at any time (via both an app and touch sensors on the robot
%itself), it will otherwise move and act on its own, without the need for
%constant supervision.

%To this end, the robot will have ground-breaking
%perception and modelling capabilities to represent the current social situation
%(the focus of WP2), coupled with an innovative cognitive architecture designed
%to combine internal social drives with domain-specific action policies learnt
%from the end-users (WP4). The robot actions themselves are designed to be
%limited to non-verbal communication mechanisms: non-verbal utterances using
%sounds, gaze, joint attention, expressive motions. In WP3, my team will work
%with a choreographer and a sound expert to create a new grammar of expressive
%motions, combined with a novel modality based on \emph{soundscapes}: sound
%landscapes that the robot can modulate to influence the mood of the social
%environment (calm, excited, worried, etc.).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%

%
%
%%The four research questions previously listed are addressed across five
%%work-packages: \textbf{WP1} is dedicated to the conceptual framing of the
%%project (R1) and the identification of interaction principles; \textbf{WP2}
%%extracts from these principles the set of requirements in term of
%%socio-cognitive capabilities for the robot (R3), and implement them; in parallel
%%to WP2,  \textbf{WP3} looks at how social robots can generate congruent social
%%behaviours (R3); \textbf{WP4} transposes the conceptual framework of WP1 into a
%%principled cognitive architecture and integrates together the cognitive
%%functions of WP2 and WP3 (R2);and \textbf{WP5} organises the experimental
%%fieldwork that demonstrates the \project approach in ambitious and complementary
%%real-world situations (R4).
%
%
%
%Figure~\ref{fig:wps} gives an overview of the project
%work packages and their interrelations. Experimental fieldwork, which plays a central role in the
%project, appears in the centre of the figure.
%
%%The first important field
%%deployment is a one-year experiment, taking place at the Bristol science centre
%%(T1.1). This `public-in-the-loop' experiment is analysed and lead to the
%%definition of core interaction principles (T1.2). These are in turn translated
%%into algorithmic models, guiding the social teleology of the cognitive
%%architecture (T4.1).
%%
%%This first experiment is immediately followed by two other long-term
%%experimental deployments: a one-year deployment in one of Bristol's Special
%%Education Need (SEN) school (T5.1), followed by a one-year deployment at
%%Bristol's Children's hospital (T5.2). These two additional experiments are both
%%inputs for WP2 and WP3, and demonstrator for the robot socio-cognitive
%%architecture (WP4).
%%
%%Specifically, work package WP2 research, develop, and integrate all the components
%%pertaining to the assessment of the spatio-temporal and social environment of
%%the robot. Reference interaction situations and the data required to support
%%this work package is directly drawn from the experimental fieldwork that will
%%take place at the same time in WP1 and WP5. The perceptual capabilities
%%delivered by WP2 are continuously integrated into the robot's cognitive
%%architecture (T4.3), iteratively improving the socio-cognitive performances of
%%the robot.
%%
%%work package WP3 looks into behaviour generation using machine learning (T3.2)
%%and non-verbal affective modalities (T3.3). T3.2 is data-intensive, and will use
%%datasets acquired during the field deployments (T1.1, T5.1, T5.2), as well as
%%lab-recorded dataset of social interactions. Similar to WP2, the capabilities
%%built in WP3 are integrated in the robot architecture in T4.3.
%%
%%In addition to the integration of WP2 and WP3 capabilities, WP4 is also
%%researching and developing the socio-cognitive drives of the architecture. They
%%come both from T1.2 (as previously mentioned), and
%%human-in-the-loop/public-in-the-loop machine learning (T4.2). T4.2, in
%%particular, is tighly connected to the experimental fieldwork, where the
%%learning-from-end-users take place.
%
%%\subsection{Integration sprints}
%%
%%\project is a complex project, with numerous interdependencies between tasks.
%%To ensure the interdependencies are properly understood, and support effective
%%integration of the outputs of each work package, I will organise every 6 months
%%\textbf{integration sprints} (see Gantt diagram). Integration sprints are
%%one-week long integration retreat during which the whole \project team gather
%%and work together to effectively implement and test on the robot the different
%%components. In addition to providing regular `check points' for the projects,
%%they also set a stable schedule to deliver project components.
%%
%%This methodology was adopted in a project the PI previously took part in (FP7
%%CHRIS project), and had proved at that time to be of great value to ensure
%%project-wide cohesion and steady progress.
%%
%%The three integration sprints taking place before the beginning of the
%%experimental deployments (display as orange circles on the Gantt chart) are of
%%particular importance, and will be extended to two weeks.
%
%\subsection{WP1: \textbf{\wpOne}}
%
%WP1 aims at establishing the conceptual and ethical framework around the idea of
%\emph{robot-supported human-human interactions}. It does so by co-creating
%patterns of interaction and norms with the general public, using a unique
%combination of ethnographic observations and `crowd-sourced' interaction
%patterns.
%
%\begin{framed}
%    \textbf{Main outcomes:} A theoretical framework for thinking about the role of
%    social robots and guidelines to inform policies for this (including ethical
%    implications); a set of operational and co-created interaction principles; a
%    large dataset of social human-robot interactions
%
%    \textbf{Timeframe:} \textbf{Y1-Y3}; one senior post-doctoral research
%    assistant (PD1) with background in the sociology of technology.
%\end{framed}
%
%
%\textbf{T1.1 -- Conceptual framing and ethics of robot-supported social
%interactions}
%
%
%The first task in WP1 is to research and define the conceptual framework around
%questions like: what role should social robots have? Where
%to set the boundaries of artificial social interactions? What do
%`ethical-by-design' and `responsible-by-design' mean in the context of social
%human-robot interactions? 
%
%Each of the field experiments (T1.2, T5.1. T5.2) will both \emph{build on} and
%\emph{feed into} the framework developed in this task. In addition, four
%two-day workshops with the \project Ethics Advisory Board, spread over the
%duration of the project, will act as milestones for ethics review.
%
%\textbf{T1.2 -- Crowd-sourced patterns of robot-supported social
%interactions} The conceptual framework identified in T1.1 is translated
%into a set of \emph{interaction design principles} and \emph{determinants} that
%will together form a set of requirements and objectives for the socio-cognitive
%capabilities and architecture developed in WP2 and WP4.
%
%In order to anchor T1.2 in the reality and complexity of human social
%interactions and involve the public in the design of these patterns and norms, I
%will embed one \project robot in the Bristol Science Centre WeTheCurious for a
%whole year (Y2). With the help of a researcher (PDRA), the visitors will be guided in
%tele-operating the robot to assist other visitors, and, by doing so, co-design
%what defines a good robot helper. This will generate the quantitative and
%qualitative data to inform questions like `what role for the robot?', `when to
%intervene?' and `what are effective and acceptable social influence
%techniques?'. It will also be a unique example of crowd-sourcing at a large
%scale, with the general public, the design of the interactions with social
%robots. The generated dataset will also be used as a data source in WP2 and
%WP3.
%
%\textbf{Specific resources} The Bristol Science Centre is fully committed to the
%project. They will include \project in its official programme of activities, and
%provide in-kind training for the \project researcher based at the centre.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% WeTheCurious
%% 
%% - one robot completelty controlled by children, one by adults
%% 
%% what to learn?
%% 
%% - when to approach? when to prompt? [example of the salesman/museum facilitator]
%% - when is the right time to help/intervene or not? 'child being told off by
%% parents -> not the right time!'
%% - group interactions -> when to intervene? what about peer-pressure? eg what if
%% I tell off one child in front of another?
%% - break the barrier for participation. Japanese Journal paper -> facilitating students questions
%% - impact on moral norms? what behaviours is acceptable?
%% - what role for the robot? another mediator? a peer?
%% - what can we do with that 'alien creature'
%% 
%% - robot taking one child to talk to the museum mediators ("I, robot, am  shy!
%% would you come with me?")
%% 
%% - learning how to adjust behaviour based on personality
%% - 'why do I behave like that with that person, and like this with that other
%% person?'
%% 
%% - reinforcement learning instead of human-in-the-loop -> what reinforcement
%% signal? engagement
%% 
%% - the robot that 'take sides': take side against the adults? -> bending in its
%% role?
%% 
%% 
%% - social embarassment
%% - space for pretence: the robot can adopt an 'artificial role' as long as it is
%% possible (accpetable/...) to pretend the robot is
%
%
%
%\subsection{WP2: \textbf{\wpTwo}}
%
%
%In WP2, the project addresses the key scientific and technical prerequisites to
%effectively deliver WP4's cognitive architecture: namely, the perception and modeling of
%the spatio-temporal and social environment of the robot. This includes spatial
%characteristics (proxemics; group dynamics; complex, dynamic attentional
%mechanisms); psycho-social determinants (social roles and hierarchies; social
%groups; mental modelling; anthropomorphic ascriptions), and temporal characteristics
%(effects of novelty; dynamics of anthropomorphism and mental ascriptions; group
%dynamics). I have investigated many of these socio-cognitive capabilities in
%isolation (Table~\ref{pi-expertise}), and this WP is about
%\emph{integrating} them into a coherent perceptual subsystem, significantly
%extending the state of the art\parencite{lemaignan2017artificial}\textsuperscript{,}\parencite{baxter2016cognitive}.
%
%\begin{framed}
%    \textbf{Main outcomes:} A complete pipeline for spatio-temporal and social
%    situation assessment, built as open-source ROS nodes and able to map in
%    real time the physical and social environment of the robot.
%
%    \textbf{Timeframe:} \textbf{Y1-Y4}; one post-doctoral research assistat (PD2) in social
%signal processing/machine learning/cognitive modelling expertise.
%\end{framed}
%
%\textbf{T2.1 -- Hybrid situation assessment and knowledge representation} This
%task builds the foundational spatio-temporal and symbolic perception and
%representation system for the robot. It will integrate the state of the art in
%spatio-temporal situation assessment that I have previously
%developed\parencite{lemaignan2018underworlds}\parencite{sallami2019simulation},
%drawing on recent
%advances in data-driven semantic labelling (for instance, using 4D convolution
%nets like MinkowskiNet\parencite{choy20194d}), and a symbolic knowledge base (like
%my own ontology-based one\parencite{lemaignan2010oro}) in order to create a coherent
%system of representations for the cognitive architecture of the robot.
%
%\textbf{T2.2 -- Multi-modal human model} This task focuses on the processing and
%modelling of social signals, extending existing techniques both
%model-based\parencite{gunes2017automatic}\textsuperscript{,}\parencite{lemaignan2016realtime} and
%data-driven\parencite{bartlett2019what}. This task goes beyond the
%state of the art by looking specifically at resolving highly dynamical signals
%(like gaze saccades and micro facial expressions). Required datasets will be
%drawn from my previous work\parencite{lemaignan2018pinsoro}, as well as from
%the project experiments (T1.2, T5.1, T5.2).
%
%\textbf{T2.3 -- Interaction and group dynamics} Building on T2.2, T2.3
%investigates the automatic understanding and modelling of group-level social
%interactions\parencite{tapus2019perceiving}, including
%$f$-formations\parencite{marshall2011using},
%sociograms\parencite{garcia2016hybrid}, and inter-personal
%affordances\parencite{pandey2013affordance}. This task builds on literature on
%social dynamics analysis
%(eg\parencite{durantin2017social}\textsuperscript{,}\parencite{jermann2009physical}\textsuperscript{,}\parencite{martinez2019collocated})
%to apply it to real-time social assessment by a robot, which is itself embedded
%in the interaction.
%
%\textbf{T2.4 -- Integrated model of the social environment} The integration of
%the social cues from T2.2 and T2.3 results in a socio-cognitive model of the
%social environment of the robot, which will effectively extend the representation
%capabilities of T2.1 to the social sphere. The result of T2.4 is an AI module
%that implements a full social assessment pipeline, from social signal perception
%to higher-level socio-cognitive constructs. T2.4 also includes a
%focused experimental programme (based on the protocols designed by Frith and
%Happ√©\parencite{frith1994autism}, which I introduce in\parencite{lemaignan2015mutual}) to
%demonstrate in isolation the resulting socio-cognitive capabilities.
%
%
%\subsection{WP3: \textbf{\wpThree}} 
%
%Mirroring WP2's focus on understanding the social interactions, WP3 addresses the
%question of social behaviour \emph{production}: how to create natural,
%non-repetitive behaviours, engaging end-users over a sustained period of time. The robot
%behaviours will be exclusively non-verbal (non-verbal utterances, gaze, joint
%attention, facial expressions and expressive motions), and will include
%soundscapes as a novel, non-verbal interaction modality.
%
%\begin{framed}
%
%    \textbf{Main outcomes:} A new method to automatically design complex and
%    non-repetitive social behaviours, with a focus on non-verbal communication;
%    research on soundscapes as a novel non-verbal modality for human-robot
%    interaction.
%
%    \textbf{Timeframe:} \textbf{Y2-Y5}; one post-doctoral research assistant (PD3) in machine learning/learning from
%demonstration.
%
%\end{framed}
%
%\textbf{T3.1 -- Behavioural baseline} T3.1 establishes a baseline for behaviour
%generation, by surveying and implementing the current state of the art
%(behaviours library, activity switching\parencite{coninx2016towards}). This
%baseline will enable early in-situ experimental deployments, while also
%providing a comparison point for T3.2.
%
%\textbf{T3.2 -- Generative neural network for social behaviour production}
%\project aims to significantly advance the state of the art in this regard, by
%combining two recent techniques: (1) generative neural networks for affective
%robot motion
%generation\parencite{yang2019appgan}\textsuperscript{,}\parencite{marmpena2019generating}\textsuperscript{,}\parencite{suguitan2020moveae}
%(with training data created with an expert choreographer); (2) interactive
%machine learning in high-dimensional input/output spaces, where I have achieved
%with my students promising results for generating complex social
%behaviours\parencite{senft2019teaching}\textsuperscript{,}\parencite{winkle2020insitu}
%that fully involve the end-users\parencite{winkle2018social}. Modulating (1)
%with the learnt features of (2), I will target a breakthrough in generating robots' social
%behaviours: the generation of non-repetitive, socially congruent and
%transparent social behaviours (including gestures but also gazing behaviours and
%facial expressions).
%
%\textbf{T3.3 -- Non-verbal behaviours and robot soundscape} In task T3.3, we
%introduce a novel non-verbal interaction modality for robots, based on
%soundscapes. Soundscapes involve creating a sound environment that reflects a
%particular situation; they have also been shown to be an effective intervention
%technique in the context of special needs
%interventions\parencite{greher2010soundscape}. The soundscapes that I will
%create are `owned' by the robot, which can manipulate them itself, for example
%to create an approachable, non-threatening, non-judgmental, social interaction
%context, or make the interaction a trusted physical and emotional safe-space for
%the user.
%
%\textbf{Specific resource}: these soundscapes will be co-designed with Dr.
%Dave Meckin, an expert on sound design for vulnerable children, and a staff
%member at the host institution.
%
%\subsection{WP4: \textbf{\wpFour}}
%
%In WP4, I will design a novel socio-cognitive architecture for social
%robots and implement it on the Halodi EVE robot. WP4 will integrate the
%modeling capabilities and behaviour production developed in WP2 and WP3, with a
%dual action policy -- one driven by a social teleology (an artificial
%intrinsic motivation to act socially) and one learned through
%human-in-the-loop machine learning (Figure~\ref{fig:archi}). This WP is high-risk/high-gain: while sustaining
%long-term engagement in a principled way remains a major scientific
%challenge in social robotics\parencite{hoffman2019anki}, the WP adopts a very novel
%approach to goal-driven socio-cognitive architectures. It has the potential to
%unlock long-term social engagement by endowing the robot with its own
%intentionality\parencite{wiese2017robots}, while maintaining human oversight.
%
%\begin{figure}[h!]
%\centering
%\includegraphics[width=0.8\linewidth]{archi.pdf}
%\caption{Overview of the AI engine implemented in \project.}
%\label{fig:archi}
%\end{figure}
%
%
%\begin{oframed}
%    \textbf{Main outcomes:} An integrated cognitive architecture for social
%    robots, driven by both long-term social goals, and machine-learnt action
%    policies; a reference open-source implementation, enabling long-term
%    autonomy for the Halodi EVE robot.
%
%    \textbf{Timeframe:} \textbf{Y1-Y5}; one post-doctoral research assistant (PD4) in cognitive
%    robotics; one PhD student (PHD1).
%
%\end{oframed}
%
%\textbf{T4.1 -- A social teleology for robots} The idea of a \emph{teleological}
%(ie goal-driven) robot architecture for social interaction is highly novel
%(existing literature on teleological robots only focuses on simple cognitive
%systems\parencite{oudeyer2005playground}\textsuperscript{,}\parencite{moulinfrier2014self})
%or virtual agents\parencite{pathak2017curiosity}.This task will design and
%implement such an architecture on the EVE robot. It first identifies
%\emph{interaction principles} from the interaction patterns and determinants
%uncovered in T1.2; these are then mapped into \emph{long-term interaction
%goals}, capable of driving the robot actions over a period of time.
%
%\textbf{T4.2 -- Learning from humans to achieve `by-design' responsible \&
%trustworthy AI} Building on my recent results on human-in-the-loop social
%learning\parencite{senft2017supervised}\textsuperscript{,}\parencite{senft2019teaching}\textsuperscript{,}\parencite{winkle2020insitu},
%this task implements the mechanics to allow human end-users to progressively
%teach the robot a domain-specific social policy.  It also qualitatively
%researches how human-in-the-loop machine learning enables a more trustworthy AI
%system by involving end-users in the creation of the robot behaviours,
%resulting in \emph{explainable} behaviours for the end-users.
%
%\textbf{T4.3 -- Integrating a socially-driven architecture for long-term
%interaction} Building on my previous work on cognitive
%architecture\parencite{lemaignan2017artificial}, this task brings together, in
%a principled manner, the perceptual (WP2) and behavioural (WP3) capabilities of
%the robot, as well as the social policies created in T4.1 and T4.2. T4.3 will
%specifically look at long term autonomy, including long-term social goals,
%cognitive redundancy and behavioural complexity.
%
%T4.3 will also develop the arbitration mechanism that combines the robot's
%social teleology (T4.1) with the human-taught action policy (T4.2). This
%arbitration mechanism will build on research on reinforcement learning for
%experience transfer\parencite{madden2004transfer} that enables the re-assessment of
%a policy (here, our intrinsic motivation) based on previous experience (here,
%the human-taught policy).
%
%\subsection{WP5: \textbf{\wpFive}}
%
%Finally, WP5 aims to convincingly demonstrate the importance and positive
%impact that socially-driven, socially-responsible robotics may have. The
%experimental work of \project will be organised around an ambitious long-term
%study, set in a complex, real-world environment, at a Special Educative
%Needs (SEN) school.
%
%This environment also put the project in the unique position of actually
%delivering high societal impact: I anticipate 250+ SEN-educated children will
%directly benefit from the project, exploring how robots can have a net social
%utility while being accepted as an effective tool by field practitioners. This
%deployment will take place within the strict ethical framework established in
%T1.1.
%
%\begin{oframed}
%
%    \textbf{Main outcomes:} One long-term deployment of a social robot in a
%    real-world, high impact environment, demonstrating long-term acceptance and
%    social utility; large (anonymous) datasets of complex, real-world
%    human-robot interactions.
%
%    \textbf{Timeframe:} \textbf{Y3-Y5}; one post-doctoral research assistant (PD4, shared with WP4).
%
%\end{oframed}
%
%\textbf{T5.1 -- A robot companion to support physical, mental and social
%well-being in SEN schools} This task aims to demonstrate robot-supported
%social interventions within Bristol's network of SEN schools.  During a one-year
%period (Y3), the robot will be based in schools, with interventions co-designed
%with the teachers, the parents and the students, both through preliminary
%focus groups and in-situ machine learning.\TODO{Add more/rephrase}
%
%The envisioned interventions include: initiating group games; asking students
%about their well-being; co-teaching material with teachers; fostering
%interactions between the children.
%
%\textbf{Specific resources:} The task will be supported by SEN researcher Dr.
%Nigel Newbutt, a staff member at the host institution, who has a long track
%record and on-going research partnerships with Bristol's special needs schools.
%I am currently collaborating with Dr. Newbutt on a pilot study which involve a
%non-autonomous (teleoperated) robot in the same SEN school.
%
%
%\textbf{T5.2 -- \TODO{is there a T5.2? Potentially add material from Robot4SEN
%proposal}}
%
%\textbf{Specific resources:}
%
%\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\section{Risk/gain assessment; risk mitigations}\label{risks}
%
%\textbf{Tasks 1.1, 1.2} develop a novel methodology, `public-in-the-loop' machine
%learning, for large-scale co-design of social interactions with the public. If
%successful, this will be of great value, well beyond the project. The
%proposed experimental setup (science centre visitors `taking control' of the robot)
%might however lead to interactions that are either too short or to artificial to
%create meaningful, generalisable social interaction. In addition, the messy and
%complex nature of the science centre environment is also currently beyond-state-of-the-art
%in term of extracting the useful social features required to train a classifier.
%
%However, the interaction principles that we want to uncover in T1.1 and T1.2
%(and that are feeding into WP2 and WP4) will principally come from a qualitative
%analysis of the interactions, carried in parallel to the machine learning
%approach. This well within the expertise of the PI, and, as such, is low-risk.
%T1.1 can thus be described as a \ul{\bf medium-risk, high-gain} component of
%\project.
%
%\vspace{1em}
%
%\textbf{Task 2.1} develops a novel situation assessment component, that
%integrates spatio-temporal modeling with knowledge representation. The resulting
%component is beyond-state-of-the-art, and would be highly relevant to a large range
%of robotic applications. This component relies on integrating tools that are
%independently relatively mature and well understood, and the principles of the
%integration itself is already well researched. Besides, it falls well within the
%PI
%expertise~\parencite{lemaignan2018underworlds,sallami2019simulation,lemaignan2010oro}.
%As such, T2.1 can be described as \ul{\bf low-risk, medium-gain}.
%
%\textbf{Tasks 2.2, 2.3, 2.4} Work on real-time modeling of social dynamics in
%real-world environments are only begining to be studied in robotics. While the
%underpinning are well understood in neighbouring academic fields, a very
%significant work remain to be done to integrate disparate or partial approaches
%into one framework. These tasks also require the acquisition of novel datasets
%that focus on natural human-human social interactions. The PI has extensive
%experience in building and acquiring such
%datasets~\parencite{lemaignan2018pinsoro,sallami2020unexpected}, and does not
%foreseen major difficulties. The resulting components have however the potential
%to unlock a new class of social robots, aware in real-time of their social
%surroundings and dynamics.  These tasks are thus considered \ul{\bf low-risk,
%high-gain}.
%
%\vspace{1em}
%
%\textbf{Task 3.1} The behavioural baseline implements the current state-of-the-art,
%and as such is \ul{\bf low-risk, low-gain}. T3.1 will guarantee early on in the
%project a `working' robot, yet with predictable/repetitive behaviours.
%
%\textbf{Task 3.2} The neural generation of complex social behaviours is a
%\ul{\bf medium-risk, high-gain} task: while it builds on solid existing
%state-of-the-art, it relies on very significant progress in both the modeling of the
%social dynamics (WP2) and the capacity of designing a machine learning approach
%to learn and generate these complex behaviours. While the former falls well
%within the PI expertise, machine learning for social motion generation is
%essentially a novel field. The success of this task will rely to a large
%extend on the quality of the post-doctoral researcher recruited to lead this
%effort. The main mitigation to the risk associated to T3.2 is the behavioural
%baseline created in T3.1: the behavioural capabilities generated in T3.2 can be
%complemented by ad-hoc behaviours whenever required.
%
%\textbf{Task 3.3} Non-verbal communication is a well established subfield of HRI
%research, well known to the PI. The creation of the novel interaction modality
%based on soundscape is novel, with potential for impact beyond the project. This
%new modality will be co-developped with an expert of sound design for
%interaction, and we do not foresee major risks. Overall, the task is \ul{\bf
%low-risk, medium-gain}.
%
%\vspace{1em}
%
%\textbf{Task 4.1} The conceptual framing of a \emph{socially-driven
%architecture} (social teleology) and its translation into decision-making
%algorithms are to a large extend open questions. This task might however lead to
%uncover a fundamental mechanism to enable long-term engagement of users
%with social robots. Building fundamentally on blue-sky research, this task is
%\ul{\bf high-risk, high-gain}. If not successful, I will instead rely on the
%decision-making strategy of T4.2, which is much lower risk.
%
%\textbf{Task 4.2} The techniques developed in T4.2 have been previously used and
%tested by the PI in two different real-world
%environments~\parencite{senft2019teaching,winkle2020couch}. While they will require
%significant adjustments for this project, the task is overall \ul{\bf low-risk,
%low-gain}.
%
%\textbf{Task 4.3} The integration of the different cognitive functions of the
%robot into one principled cognitive architecture, that include cognitive
%redundancy, is one of the core expertise of the
%PI~\parencite{lemaignan2017artificial}. This task however includes significant novel
%elements (cognitive mechanisms for long-term autonomy; decision arbitration)
%that bear unknowns. Besides, this task is a critical pre-requisite for WP5. As a
%result, T4.3 is considered as \ul{\bf high-risk}. The task is focused on
%integration to meet the requirements of the WP5 experiments, and parts
%of the resulting software architecture might be project-specific. However the
%overall aims of endowing the robot with long-term social autonomy would be a
%significant breakthrough, and as such, T4.3 is \ul{\bf high-gain}. The main
%mitigations comes from (1) the iterative development process of the
%architecture, that will start from the existing state-of-the-art, to which the
%PI has previously contributed~\parencite{lemaignan2017artificial}. By doing so, a
%decisional architecture for the robot will be available early on in the project.
%While that architecture might be a scaled-down version of the initial ambition,
%it will still enable the fieldwork proposed in WP5, possibly with a lesser level
%of autonomy; (2) the possibility of using only one of the two action policies
%(T4.1 \ul{or} T4.2), thus removing the need for complex arbitration.
%
%\vspace{1em}
%
%\textbf{WP5: Experimental deployments}
%
%The two application scenarios (at the children hospital and in the SEN school)
%are ambitious and inherently risky, as they target vulnerable populations.
%However, first, demonstrating the importance of advanced social modelling, and
%convincingly proving the effectiveness of our approach does require accordingly
%complex social situations, and complex social dynamics. The two scenarios, which
%complement each other, provide both.
%
%Second, working with vulnerable populations, in constrained and complex
%environments (children hospital and SEN schools) adds significant risks to the
%project. But it is also what make the project in the unique position of
%delivering a high societal impact: a direct positive impact on children's lives
%(we anticipate 100+ hospitalised children and 50+ children with psycho-social
%impairements interacting over long periods of time with a robot over the course
%of the project), and a broader impact on the society, showing how robots can
%have a lasting, strong, positive impact on the society, also establishing the
%idea of \emph{robots supporting human interactions} instead of dehumanising our
%social relationships.
%
%\textbf{Together, Task 5.1 and 5.2 are \ul{high-risk, high-gain}.}
%
%The two main mitigations are (1) early and continuous engagement with the
%stakeholders, and (2) the decoupling of the two applications, meaning that the
%risks associated to each of them do not impact the other one.
%
%Early engagement will be ensured by relying on a participatory design
%methodology, involving all the stakeholders from the onset of the project; the
%methodology will involve regular joint workshops; on-site (hospital and SEN
%schools) research stay including engagement with the staff/charities and the
%children themselves; early field testing and prototyping, relying if necessary
%on provisional, yet well-known, robot platforms available at the host
%institution (for instance, Softbank Nao and Pepper). This user-centered approach
%will be championed by the post-doc recruited on the project on WP4 and WP5, who will
%have to have a strong expertise in user-centered design.
%
%It is also important to note that, while preparing this bid, initial discussions
%have been held with all the partners involved with the experimental fieldwork
%(WeTheCurious science centre, Bristol's Children Hospital, the network of SEN
%schools): each of these institutions is enthusiastic about the project, already
%contributing ideas to integrate the robots in their daily routines, and
%ready to dedicate time and effort for its success.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\newpage
%
%\section{Appendix: Current research grants and any on-going applications related
%to the proposal}
%
%\subsection{Current Grants}
%
%\begin{tabular}{p{1.8cm}p{1.8cm}llp{4cm}p{4cm}}
%\toprule
%\textbf{Project Title} & \textbf{Funding source} & \textbf{Amount} & \textbf{Period} & \textbf{Role of the PI} & \textbf{Relation to current  ERC proposal} \\ \midrule
%    CAPRI & InnovateUK (UK) & ‚Ç¨4 840 508 & 2017 -- 2020 & Co-I for BRL; driverless car simulation for safety verification & Dev. and verification of trustworthy autonomous systems \\ \midrule
%    ROBOPILOT & InnovateUK (UK) & ‚Ç¨7 986 981 & 2018 -- 2020 & Co-I for BRL; driverless car simulation for safety verification & Dev. and verification of trustworthy autonomous systems \\ \midrule
%    CAV Forth & InnovateUK (UK) & ‚Ç¨5 093 327 & 2019 -- 2021 & Co-I for BRL; supervising the safety case and simulation-based verification & Dev. and verification of trustworthy autonomous systems \\ \midrule
%    RoboClass & UWE (UK) & ‚Ç¨5 854 & 2019 -- 2020 & PI; project supervision and robot development & Classroom deployment of a social robot \\ \bottomrule
%\end{tabular}
%
%\subsection{On-going and submitted grant applications}
%
%\begin{tabular}{p{1.8cm}p{1.8cm}llp{4cm}p{4cm}}
%\toprule
%\textbf{Project Title} & \textbf{Funding source} & \textbf{Amount} & \textbf{Period} & \textbf{Role of the PI} & \textbf{Relation to current  ERC proposal} \\ \midrule
%    RoboPets & Amazon (US) & ‚Ç¨10 942 & 2020 -- 2020 & PI; project supervision and lead researcher & Learning and generation of continuous, congruent social behaviours \\ \midrule
%    ROBUST & EPSRC (UK) & ‚Ç¨761 124 & 2020 -- 2022 & PI; project supervision and
%    architecture implementation & Dev. of a redundant cognitive robot architecture for HRI \\ \midrule
%    HEROS & H2020 (EU) & ‚Ç¨7M & 2021 -- 2024 & Co-I for BRL; research
%    lead on cognitive architecture & Dev. of a redundant cognitive robot architecture for HRI \\ \midrule
%    Robots4SEN & UWE (UK) & ‚Ç¨29 274 & 2020 -- 2021 & PI; project supervision and robot development & Pilot deployment of a social robot in a SEN school \\ \bottomrule
%\end{tabular}
%



%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\newpage
%\chapter{B2.a State-of-the-art and objectives}
%
%\eu{(B2.a, B2.b, B2.c: max 15 pages (2 pages for B2.c)}
%\eu{Specify the proposal objectives in the context of the state
%of the art in the research field. It should be clear how and why the proposed work is important for
%the field, and what impact it will have if successful, such as how it may open up new horizons or
%opportunities for science, technology or scholarship. Specify any particularly challenging or
%unconventional aspects of the proposal, including multi- or inter-disciplinary aspects.}
%
%\TODO{TARGET PAGE COUNT: 3 pages: state of art + vision; 2 pages: methodology overview; 5 pages:
%WPs; 3 pages: ethics + risks; 2 pages: resources}
%
%\TODO{as a reference: DECRESIM project: 4 pages on B2.a State of art and
%objectives; B2.b ~7 pages on WPs + 2 pages on risk assessment}
%
%
%
%\section{State of the art: real-world social robots and impact on the
%society}
%
%Social robotics is a disruptive field, with a profound impact on society and
%economy\parencite{williams2020social}. A recent report from the United Nations about
%the impact of the technological revolution on labour markets stated that AI and
%robotics are expected to radically change the labor market world-wide destroying
%some job categories and creating others\parencite{bruckner2017frontier}. Social
%robotics, however, is still an young, emerging, research-active field. The
%expectations are high, in multiple application domains: elderly care, customer
%service (in airports and shopping malls, for instance), education, child
%development, and autonomous vehicles to name a few\parencite{baillie2019challenges}.
%However, whereas both computer-based AI applications, and traditional industrial
%robots already have a significant economic impact, social robots have not
%reached that point yet. Significantly, the recent failures of several companies
%investing in social robotics, like Jibo, Kuri, Willow Garage and Anki, and the
%major setbacks of companies like SoftBank, who designed and deployed hundreds of
%Pepper robot in their shops, before renouncing a few months later due to the poor
%reception by the customers, show that these technologies are not yet
%mature\parencite{tulli2019great}.
%
%Indeed, understanding \emph{why} these robots have failed, is one of the
%active debate within the Human-Robot Interaction
%community\parencite{hoffman2019anki}, with only a handful of qualitative studies on
%this question\parencite{dereshev2019longterm,degraaf2017phased}. Proposed
%explanations include the lack of perceived usefulness (robot seen purely as a
%toy); the limited liveliness of the robot that become rapidly predictable and
%repetitive\parencite{lemaignan2014cognitive}; the poor management of expectations,
%where user over-ascribe cognitive capabilities that do not match the reality.
%The community agrees however that the crux of the issue is achieving long-term
%social engagement\parencite{yang2018grand,hoffman2019anki}
%
%Research is however seemingly hitting a wall to further progress towards
%socially meaningful long-term interactions. For instance, in their large review
%of research in robotics for education, Belpaeme et al.\parencite{belpaeme2018social}
%point to the shortcomings that prevent further development of effective,
%long-term social robotics in educative settings: the need for a correct
%interpretation of the social environment; the difficulty of action selection;
%the difficulty of pacing generated behaviours: three issues that underpin
%long-term engagement.
%
%Attempts at long-term human-robot interactions are nevertheless becoming more
%common\parencite{kunze2018artificial,leite2013social}, with a number of studies
%involving social robots deployed in real-world settings (for instance in
%schools\parencite{leite2014empathic,westlund2017measuring,
%lemaignan2016learning,coninx2016towards}, homes\parencite{degraaf2017phased} and
%care centres\parencite{hawes2017strands,winkle2020insitu}) over relatively long
%periods of time (up to 2 or 3 months at a time). Even though these robots are
%typically not fully autonomous, they do exhibit a level of autonomy, either by
%handling autonomously a relatively broad range of shallow tasks (eg, a
%butler-like robot answering simple questions, like in\parencite{hawes2017strands} or
%in the H2020 MuMMer\parencite{heikkila2018can} and FP7
%Spencer\parencite{triebel2016spencer} projects), or a narrow, well-specified complex
%task (for instance, supporting exercising in a gym, as I did in\parencite{winkle2020insitu}).
%However, general purpose, long-term interaction is still an open question.
%
%
%\subsection{Social robotics and vulnerable children}
%
%Application of social robotics to vulnerable children (either hospitalised or
%suffering cognitive and/or motor impairments) is an active field of research.
%This reflects both the measured efficacy of robot-based intervention, and the
%perceived need for additional support for these populations.  Several European
%projects have looked at these populations, for instance the FP7 Aliz-e and H2020
%DREAM projects: in Aliz-e, \parencite{baxter2011long,coninx2016towards} report on how
%a social robot can support long-term engagement with diabetic children in
%hospital (noting however the rather ``crude'' nature of the created social
%interactions, and the limited autonomy of the robot).
%
%A significant body of literature also exist on robotics and autism (see for
%instance review by Pennisi et al.\parencite{pennisi2016autism}). This specific
%domain has been a fruitful terrain to explore specific aspects of social
%cognition in robotics (for instance, related to the Theory of Mind or to the
%processing of emotions. See my review in\parencite{lemaignan2015mutual}). This
%research draws on the extensive prior research in experimental cognitive
%sciences on autism (for instance\parencite{baron1985does, frith1994autism}), and the
%focused experimental programme of WP2 will specifically draw from this body of
%prior work to evidence the newly developed social modeling capabilities of the
%robot.
%
%The experimental programme of \project will take place over two years, first in
%Special Education Needs schools (WP5.1), then in a paediatric ward at the
%Bristol's Children's Hospital (WP5.2). For both these studies, I will build on
%the experience learnt from previous studies in similar environments, with the
%novel contributions being both the very long-term interventions (one year each),
%and the user-centered methodology that I describe in the following sections.
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section{\project aim and objectives: Responsible robots for long-term social engagement}
%
%%\begin{figure}
%%    \centering
%%    \includegraphics[width=0.9\linewidth]{figs/wizme+dolls}
%%    \caption{Early prototyping for the \project project: a pet-like robot that could
%%    mediate child-child interactions, and support group activities like active story-telling.}
%%    \label{early-prototype}
%%\end{figure}
%
%
%
%%
%%\begin{figure}
%%\centering
%%\includegraphics[width=0.9\linewidth]{figs/rHHI-1}
%%\caption{A child, just arriving in a new school, tries to integrate with other
%%    children -- in such a situation, should the robot decide to help to break the ice? What
%%    are the socio-cognitive peceptual and behavioural capabilities required to
%%    make that decision?}
%%\label{fig:rHHI}
%%\end{figure}
%%
%
%The overall aim of the \project project is to \textbf{create, sustain and better
%understand the dynamics of responsible long-term social human-robot
%interactions}.
%
%This broad aim translates in three key research questions that we seek to
%address over the course of the project:
%
%\paragraph{\bf RQ1:} what are the public expectations with respect to the role of social
%        robots? Can we collectively design principles ensuring safe, beneficial, socially
%        acceptable robots? 
%
%\paragraph{\bf RQ2:} what AI is required to sustain long-term engagement between end-users
%        and a robot? In particular, how to provide a robot with an understanding
%        of its own social environment? How to create behaviours that are not
%        repetitive or overly predictable?
%
%\paragraph{\bf RQ3:} what new ethical questions are raised by long-term social interaction
%        with an artificial agent? In particular, how to balance autonomy of the
%        robot with behaviour transparency and human oversight?
%
%\vspace{0.5em}
%\noindent The \project objectives are built around these three questions.
%
%\paragraph{\bf O1: conceptual framing} I will investigate the basic principles
%of responsible social interactions, that must form the foundations of a socially
%useful robot, accepted and used in the long run. I will answer questions like:
%What should motivate the robot to step in and attempt to help? What social norms
%are applicable to the robot behaviours? Using user-centred design and
%participatory design methodologies, I will identify the determinants and
%parameters of a responsible social intervention, performed by a socially-driven
%robot, and formalise them in guidelines. This objective aims at addressing RQ1,
%and is realised in WP1.
%
%\paragraph{\bf O2: real-time social modeling} I will significantly extend and integrate
%the current state-of-art in spatio-temporal modeling (so-called \emph{situation
%assessment}) with recent research in social state modeling, to create the novel
%cognitive capability of artificial \emph{social situation assessment}, enabling
%the robot to represent in real-time the social dynamics of its environment. This
%objective addresses one part of RQ2, and is investigated in WP2.
%
%
%\paragraph{\bf O3: congruent social behaviours production} Using the
%state-of-the-art in generative neural networks, combined with data acquired from
%an expert choreographer, I will create a novel way of producing non-repetitive,
%socially-congruent, expressive motions. This will be integrated with novel
%\emph{sound landscapes} to create a beyond-state-of-art, non-verbal yet highly
%expressive,  action and communication system for the robot. This objective
%addresses another part of RQ2, and is the focus of WP3.
%
%\paragraph{\bf O4: embodied AI breakthrough} I will integrate long-term social
%goals, arising from the interaction principles of \textbf{O1}, with the social
%modeling capability of \textbf{O2} and the behaviours production of \textbf{O3}
%into a principled, goal-driven cognitive architecture. The breakthrough will
%come from combining these long-term social goals with bottom-up action policies,
%designed and learnt from the end-users using human-in-the-loop reinforcement learning.
%This will result in robot behaviours that are perceived as purposeful and
%intentional (long-term goals), while being shaped by a
%user-created and user-controlled action policy.
%
%I will specifically test \ul{two hypotheses}: first, I hypothesise that long-term
%social goals, if suitably co-designed with the public and stakeholders, and
%properly integrated into the robot as a \emph{social teleology}, will create the
%perception that the robot is intentional and purposeful. This will in turn
%elicit sustained engagement from its human users.
%
%Second, I hypothesise that human-in-the-loop machine learning can be used to
%ensure an additional layer of human oversight and a level of behavioural
%transparency.  Human-in-the-loop reinforcement learning -- as implemented in the SPARC
%approach that I have developed and already used in complex social
%environments\parencite{senft2017supervised, senft2019teaching, winkle2020insitu} -- relies
%on a end-user `teacher', initially fully controlling the robot (teleoperation)
%while it learns the action policy, and then progressively relinquishing control
%up to a point where the robot is effectively autonomous. As argued
%in\parencite{senft2019teaching}, this approach leads to increased control and
%ownership of the system, and as a result, increased trust.
%
%This addresses RQ2 and RQ3; however it also raise an additional question: how to
%arbitrate between a top-down action policy arising from the long-term goals, and
%the bottom-up action policy learnt from the end-users? This question leads to
%objective {\bf O4'}: The design of a policy arbitration mechanism that preserve
%the robot's long-term intentional behaviour, while effectively guaranteeing human
%control, ownership and oversight. {\bf O4} and {\bf O4'} are addressed in WP4.
%
%\paragraph{\bf O5: ambitious field research} Finally, the last objective of the
%\project project is to demonstrate the effectiveness of my approach in complex,
%real world conditions. This means deploying the \project robots into existing
%social eco-systems that are sufficiently complex, yet open to explore novel
%social interactions. My objective is also to show that this real world
%deployment can be successfully driven by `end-to-end' involvement of all the
%end-users and stakeholders: from defining the robot role, from the different
%perspective of each of the end-users, to actually designing and `teaching' the
%robot what to do. This is the focus of WP5.
%
%
%\begin{framed}
%
%\noindent\bf Together, these five objectives build a coherent and realistic pathway towards
%addressing the overall aim of \project: creating, sustaining and better
%understanding the dynamics of responsible long-term social human-robot
%interactions.
%
%\end{framed}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\subsection{The case for robot-supported human-human interactions}
%%
%%This change of paradigm (rHHI) will have far reaching impact on the place that
%%we collectively assign to social robots in the society; it will help to
%%structure the public debate by providing the framework to look at human-robot
%%interactions in term of their net social utility.  As such, \textbf{the first
%%outcome of \project will be researching, defining and implementing the
%%conceptual framework that we need to ensure trustworthy and socially responsible
%%robots in our societies}.
%%
%%Indeed, in a time where robots are on the verge of becoming pervasive in our
%%daily human environment, can we ensure 'by design' that robots are to become
%%powerful tools to support more and better, positive interactions \emph{between
%%the people themselves}, and build a stronger, more cohesive society? Beyond
%%Human-Computer Interaction (HCI) and Human-Robot Interaction (HRI), \project is
%%a forward-looking, ground-breaking project that aims at establishing
%%\textbf{Robot-supported Human-Human Interactions (r-HHI)} as the next step
%%toward a Responsible AI: \textbf{the scientific investigation of how social
%%robots can create, shape and support strong, sustained, positive
%%\emph{human-human} relationships}.
%%
%%As a researcher who has been working for the last 12 years in the field of
%%human-robot interaction (and child-robot interaction in particular), I have been
%%a direct witness -- by being one of the architects -- of the crossing of a
%%critical milestone: the emergence of \textbf{long-term social interactions}
%%between robots and humans. Over the last two years in particular, we observe an
%%explosion of the number of studies involving social robots, deployed in
%%real-world settings (schools, care centres) over relatively long periods of time
%%(up to 2 or 3 months at a time)\parencite{kunze2018artificial,leite2013social}. Even
%%though these robots are rarely fully autonomous, they do already show high
%%levels of autonomy\parencite{senft2019teaching}, with full autonomy in
%%sight\parencite{hawes2017strands}.
%%
%%Due to the complex interplay between the socio-psychological determinants of the
%%interactions, the technical implementation, and the multiple ethical mechanisms
%%that have to be built-in the system, it is difficult to build one coherent and
%%consistent perspective on the whole question. Indeed, the conceptual,
%%intellectual framework emcompassing both the internal cognitive mechanisms
%%required by socially intelligent robots, as well as their role and impact in the
%%society, only exists in fragmented, disconnected pieces\parencite{citeneeded}.
%%
%%
%%The lack of such a proper conceptual framing is a critical issue: for robots to
%%have a positive impact on the society, with strict ethical and privacy-related
%%safeguarding, it is urgent that the wider academic and intellectual communities,
%%beyond technologists, embrace this question and build up the debate on the
%%acceptability of robots in our society. And \textbf{this also calls for a major
%%re-thinking of the traditional paradigm of Human-Robot Interaction}.
%%Tradionally, individual cognitive functions (like natural language processing,
%%emotion recognition, proxemics-aware navigation) are implemented in robots, with
%%the ill-supported assumption that 'the more available functions, the more
%%socially-capable the robot'. This assumption is questionable, and, at any rate,
%%does not address the 'why': why does the robot decides to do what it does? What
%%drives the behaviour of the robot? The case for \emph{teleological} (ie
%%goal-driven) robotic architectures has been made in the
%%past\parencite{wrede2012towards}, but only effectively realised for relatively
%%simple cognitive systems (like curiosity-driven robot
%%animals\parencite{oudeyer2005playground} or motor babbling in infant-like
%%robots\parencite{forestier2017unified}). However, socially-driven robots,
%%participating in complex interactions with humans, have been barely
%%investigated. We need to create a new social purpose, a new social teleology to
%%drive the development of social robots.  Indeed, \textbf{being socially-driven
%%to do 'good' is essential in ensuring trustworthy, socially responsible robots}.
%%Nutrured by decades of research in understanding human social cognition and its
%%social motivations, one of the key purpose of \project is to \textbf{research
%%and build a principled and socially-driven cognitive architecture} for
%%tomorrow's social robots. This socially-driven, teleological architecture,
%%intrinsically designed to \textbf{support stronger, positive human-human
%%interactions} is what underpins the concept of \emph{robot-supported human-human
%%interactions}.
%
%%\subsection{Key scientific challenges and research questions}
%%
%%Socially intelligent robots require unique, beyond state-of-the-art,
%%capabilities to \emph{(1)} understand the social interactions (social
%%situation awareness), \emph{(2)} autonomously decide the best course of action for
%%short-term and longer-term social influence, and \emph{(3)} perform the
%%appropriate social actions and exert said influence in an appropriate,
%%responsible manner.
%%Not only the required technology is itself beyond state-of-the-art (and will be
%%researched and integrated in WP2, WP4 and WP3), but the
%%interplay between technology, socio-cognitive psychology, privacy and ethics is
%%only starting to be researched and understood. \project offers an
%%strong vision and an ambitious, evidenced-based, methodology to significantly
%%advance our understanding of this multi-faceted problem.
%%
%%Over the course of 5 years, I will investigate hypotheses H1 and H2
%%by addressing the following research questions:
%%
%%\begin{itemize}
%%    \item \textbf{R1} [conceptual framing]: what are the basic principles of
%%        responsible social interactions, that must form the foundations of a
%%        socially useful robot, accepted and used in the long run? What should
%%        motivate the robot to step in and attempt to help? What are the
%%        determinants and parameters of a social intervention, performed by a
%%        socially-driven robot, to support positive human-human social
%%        interactions? How to balance social utility and social responsibility?
%%
%%    \item \textbf{R2} [implementation level]: how will these principles
%%        be integrated into a principled, socially-driven teleological
%%        architecture for autonomous robots? How this should be combined with
%%        bottom-up action policies, designed and learnt from the end-users? How
%%        can we ensure `by design' that the resulting AI will generate useful yet
%%        responsible, trustworthy, human-centered robot behaviour?
%%
%%    \item \textbf{R3} [technology level]: where are the technological gaps in
%%        artificial social modeling and cognition, that prevent the actual
%%        realisation of a robot capable of effective social support, sustained
%%        over long period of time? How can we fill them?
%%
%%    \item \textbf{R4} [experimental level]: can we demonstrate in complex, real
%%        world conditions, the effectiveness and usefulness of the
%%        robot-supported human-human interaction paradigm? Can we do so by
%%        involving the end-users at every stage of the design, implementation and
%%        testing cycle?
%%
%%\end{itemize}
%%
%%
%%
%%\project is also a highly technical project, who aims at significantly
%%pushing the state of the art in autonomous social robotics. Indeed, in \project, we will
%%\textbf{implement the AI required for robots to effectively support
%%human social interactions}.  In that sense, this research is also
%%ground-breaking in regards to its technical objectives. In \project, robots will
%%be able to understand complex social dynamics, and generate appropriate social
%%responses, in a fully autonomous way.  Extending the current line of research of
%%the PI, we will identify, implement, and integrate the a broad range of cognitive functions into a
%%principled, socially-driven, and trustworthy socio-cognitive architecture for
%%robots. This is the second major expected scientific outcome of \project.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\section{Impact of the \project project}
%%\TODO{THIS SECTION IS STILL WORK-IN-PROGRESS}
%%
%%Academically, the \project project represents a timely combination of
%%very recent advances in supervised machine learning for social robot
%%behaviour with a creative and interdisciplinary approach to the design
%%and automation of social robot behaviour. 
%%We will publish \project results in interdisciplinary and high-profile
%%discipline-specific journals (eg. Science Robotics; Frontiers in AI and
%%Robotics; Transaction in Human-Robot Interaction) and conferences (eg. AAAI,
%%HRI, RSS).
%%
%%The dataset of social behaviours and social signals we will create and
%%distribute represents a one-in-a-kind resource for the human robot
%%interaction community, and the human data collection will be
%%transferable to research in other domains such as human-computer
%%interaction.
%%
%%As \project will be deployed in a living lab environment, there is
%%significant scope for public outreach/engagement and media coverage,
%%which we will work with the BRL's media manager to maximise.
%%
%%
%%\project aims at building unique European capacity to assert leadership in this
%%domain, and, beyond the specific deliverables of this 5-years project,
%%establishing the PI as a world-leader in goal-driven, socially-responsible
%%robotics.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage
%
%{\let\clearpage\relax\chapter{B2.b Methodology}\label{research-methodology}} % prevent page break before chapter
%
%\eu{Describe the proposed methodology in detail including any key intermediate
%goals. Explain and justify the methodology in relation to the state of the art,
%and particularly novel or unconventional aspects addressing the
%'high-risk/high-gain' balance. Highlight any intermediate stages where results
%may require adjustments to the project planning. In case you ask that team
%members are engaged by another host institution their participation has to be
%fully justified by the scientific added value they bring to the project.}
%
%
%\section{work packages overview and interrelations}\label{work package-interrelations}
%
%
%The four research questions previously listed are addressed across five
%work-packages: \textbf{WP1} is dedicated to the conceptual framing of the
%project (R1) and the identification of interaction principles; \textbf{WP2}
%extracts from these principles the set of requirements in term of
%socio-cognitive capabilities for the robot (R3), and implement them; in parallel
%to WP2,  \textbf{WP3} looks at how social robots can generate congruent social
%behaviours (R3); \textbf{WP4} transposes the conceptual framework of WP1 into a
%principled cognitive architecture and integrates together the cognitive
%functions of WP2 and WP3 (R2);and \textbf{WP5} organises the experimental
%fieldwork that demonstrates the \project approach in ambitious and complementary
%real-world situations (R4).
%
%
%
%\begin{figure}[h!]
%\centering
%\includegraphics[width=\linewidth]{figs/wps}
%\caption{Overview of the work packages and tasks, and tasks inter-relations.}
%\label{fig:wps}
%\end{figure}
%
%More specifically, Figure~\ref{fig:wps} gives an overview of the project
%work packages, and their interrelations. Fieldwork plays a central role in the
%project, and appears in the centre of the figure. The first important field
%deployment is a one-year experiment, taking place at the Bristol science centre
%(T1.1). This `public-in-the-loop' experiment is analysed and lead to the
%definition of core interaction principles (T1.2). These are in turn translated
%into algorithmic models, guiding the social teleology of the cognitive
%architecture (T4.1).
%
%This first experiment is immediately followed by two other long-term
%experimental deployments: a one-year deployment in one of Bristol's Special
%Education Need (SEN) school (T5.1), followed by a one-year deployment at
%Bristol's Children's hospital (T5.2). These two additional experiments are both
%inputs for WP2 and WP3, and demonstrator for the robot socio-cognitive
%architecture (WP4).
%
%Specifically, work package WP2 research, develop, and integrate all the components
%pertaining to the assessment of the spatio-temporal and social environment of
%the robot. Reference interaction situations and the data required to support
%this work package is directly drawn from the experimental fieldwork that will
%take place at the same time in WP1 and WP5. The perceptual capabilities
%delivered by WP2 are continuously integrated into the robot's cognitive
%architecture (T4.3), iteratively improving the socio-cognitive performances of
%the robot.
%
%work package WP3 looks into behaviour generation using machine learning (T3.2)
%and non-verbal affective modalities (T3.3). T3.2 is data-intensive, and will use
%datasets acquired during the field deployments (T1.1, T5.1, T5.2), as well as
%lab-recorded dataset of social interactions. Similar to WP2, the capabilities
%built in WP3 are integrated in the robot architecture in T4.3.
%
%In addition to the integration of WP2 and WP3 capabilities, WP4 is also
%researching and developing the socio-cognitive drives of the architecture. They
%come both from T1.2 (as previously mentioned), and
%human-in-the-loop/public-in-the-loop machine learning (T4.2). T4.2, in
%particular, is tighly connected to the experimental fieldwork, where the
%learning-from-end-users take place.
%
%\subsection{Integration sprints}
%
%\project is a complex project, with numerous interdependencies between tasks.
%To ensure the interdependencies are properly understood, and support effective
%integration of the outputs of each work package, I will organise every 6 months
%\textbf{integration sprints} (see Gantt diagram). Integration sprints are
%one-week long integration retreat during which the whole \project team gather
%and work together to effectively implement and test on the robot the different
%components. In addition to providing regular `check points' for the projects,
%they also set a stable schedule to deliver project components.
%
%This methodology was adopted in a project the PI previously took part in (FP7
%CHRIS project), and had proved at that time to be of great value to ensure
%project-wide cohesion and steady progress.
%
%The three integration sprints taking place before the beginning of the
%experimental deployments (display as orange circles on the Gantt chart) are of
%particular importance, and will be extended to two weeks.
%

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Risk/gain assessment; risk mitigations}\label{risks}
%
%\textbf{Tasks 1.1, 1.2} develop a novel methodology, `public-in-the-loop' machine
%learning, for large-scale co-design of social interactions with the public. If
%successful, this will be of great value, well beyond the project. The
%proposed experimental setup (science centre visitors `taking control' of the robot)
%might however lead to interactions that are either too short or to artificial to
%create meaningful, generalisable social interaction. In addition, the messy and
%complex nature of the science centre environment is also currently beyond-state-of-the-art
%in term of extracting the useful social features required to train a classifier.
%
%However, the interaction principles that we want to uncover in T1.1 and T1.2
%(and that are feeding into WP2 and WP4) will principally come from a qualitative
%analysis of the interactions, carried in parallel to the machine learning
%approach. This well within the expertise of the PI, and, as such, is low-risk.
%T1.1 can thus be described as a \ul{\bf medium-risk, high-gain} component of
%\project.
%
%\vspace{1em}
%
%\textbf{Task 2.1} develops a novel situation assessment component, that
%integrates spatio-temporal modeling with knowledge representation. The resulting
%component is beyond-state-of-the-art, and would be highly relevant to a large range
%of robotic applications. This component relies on integrating tools that are
%independently relatively mature and well understood, and the principles of the
%integration itself is already well researched. Besides, it falls well within the
%PI
%expertise\parencite{lemaignan2018underworlds,sallami2019simulation,lemaignan2010oro}.
%As such, T2.1 can be described as \ul{\bf low-risk, medium-gain}.
%
%\textbf{Tasks 2.2, 2.3, 2.4} Work on real-time modeling of social dynamics in
%real-world environments are only begining to be studied in robotics. While the
%underpinning are well understood in neighbouring academic fields, a very
%significant work remain to be done to integrate disparate or partial approaches
%into one framework. These tasks also require the acquisition of novel datasets
%that focus on natural human-human social interactions. The PI has extensive
%experience in building and acquiring such
%datasets\parencite{lemaignan2018pinsoro,sallami2020unexpected}, and does not
%foreseen major difficulties. The resulting components have however the potential
%to unlock a new class of social robots, aware in real-time of their social
%surroundings and dynamics.  These tasks are thus considered \ul{\bf low-risk,
%high-gain}.
%
%\vspace{1em}
%
%\textbf{Task 3.1} The behavioural baseline implements the current state-of-the-art,
%and as such is \ul{\bf low-risk, low-gain}. T3.1 will guarantee early on in the
%project a `working' robot, yet with predictable/repetitive behaviours.
%
%\textbf{Task 3.2} The neural generation of complex social behaviours is a
%\ul{\bf medium-risk, high-gain} task: while it builds on solid existing
%state-of-the-art, it relies on very significant progress in both the modeling of the
%social dynamics (WP2) and the capacity of designing a machine learning approach
%to learn and generate these complex behaviours. While the former falls well
%within the PI expertise, machine learning for social motion generation is
%essentially a novel field. The success of this task will rely to a large
%extend on the quality of the post-doctoral researcher recruited to lead this
%effort. The main mitigation to the risk associated to T3.2 is the behavioural
%baseline created in T3.1: the behavioural capabilities generated in T3.2 can be
%complemented by ad-hoc behaviours whenever required.
%
%\textbf{Task 3.3} Non-verbal communication is a well established subfield of HRI
%research, well known to the PI. The creation of the novel interaction modality
%based on soundscape is novel, with potential for impact beyond the project. This
%new modality will be co-developped with an expert of sound design for
%interaction, and we do not foresee major risks. Overall, the task is \ul{\bf
%low-risk, medium-gain}.
%
%\vspace{1em}
%
%\textbf{Task 4.1} The conceptual framing of a \emph{socially-driven
%architecture} (social teleology) and its translation into decision-making
%algorithms are to a large extend open questions. This task might however lead to
%uncover a fundamental mechanism to enable long-term engagement of users
%with social robots. Building fundamentally on blue-sky research, this task is
%\ul{\bf high-risk, high-gain}. If not successful, I will instead rely on the
%decision-making strategy of T4.2, which is much lower risk.
%
%\textbf{Task 4.2} The techniques developed in T4.2 have been previously used and
%tested by the PI in two different real-world
%environments\parencite{senft2019teaching,winkle2020insitu}. While they will require
%significant adjustments for this project, the task is overall \ul{\bf low-risk,
%low-gain}.
%
%\textbf{Task 4.3} The integration of the different cognitive functions of the
%robot into one principled cognitive architecture, that include cognitive
%redundancy, is one of the core expertise of the
%PI\parencite{lemaignan2017artificial}. This task however includes significant novel
%elements (cognitive mechanisms for long-term autonomy; decision arbitration)
%that bear unknowns. Besides, this task is a critical pre-requisite for WP5. As a
%result, T4.3 is considered as \ul{\bf high-risk}. The task is focused on
%integration to meet the requirements of the WP5 experiments, and parts
%of the resulting software architecture might be project-specific. However the
%overall aims of endowing the robot with long-term social autonomy would be a
%significant breakthrough, and as such, T4.3 is \ul{\bf high-gain}. The main
%mitigations comes from (1) the iterative development process of the
%architecture, that will start from the existing state-of-the-art, to which the
%PI has previously contributed\parencite{lemaignan2017artificial}. By doing so, a
%decisional architecture for the robot will be available early on in the project.
%While that architecture might be a scaled-down version of the initial ambition,
%it will still enable the fieldwork proposed in WP5, possibly with a lesser level
%of autonomy; (2) the possibility of using only one of the two action policies
%(T4.1 \ul{or} T4.2), thus removing the need for complex arbitration.
%
%\vspace{1em}
%
%\textbf{WP5: Experimental deployments}
%
%The two application scenarios (at the children hospital and in the SEN school)
%are ambitious and inherently risky, as they target vulnerable populations.
%However, first, demonstrating the importance of advanced social modelling, and
%convincingly proving the effectiveness of our approach does require accordingly
%complex social situations, and complex social dynamics. The two scenarios, which
%complement each other, provide both.
%
%Second, working with vulnerable populations, in constrained and complex
%environments (children hospital and SEN schools) adds significant risks to the
%project. But it is also what make the project in the unique position of
%delivering a high societal impact: a direct positive impact on children's lives
%(we anticipate 100+ hospitalised children and 50+ children with psycho-social
%impairements interacting over long periods of time with a robot over the course
%of the project), and a broader impact on the society, showing how robots can
%have a lasting, strong, positive impact on the society, also establishing the
%idea of \emph{robots supporting human interactions} instead of dehumanising our
%social relationships.
%
%\textbf{Together, Task 5.1 and 5.2 are \ul{high-risk, high-gain}.}
%
%The two main mitigations are (1) early and continuous engagement with the
%stakeholders, and (2) the decoupling of the two applications, meaning that the
%risks associated to each of them do not impact the other one.
%
%Early engagement will be ensured by relying on a participatory design
%methodology, involving all the stakeholders from the onset of the project; the
%methodology will involve regular joint workshops; on-site (hospital and SEN
%schools) research stay including engagement with the staff/charities and the
%children themselves; early field testing and prototyping, relying if necessary
%on provisional, yet well-known, robot platforms available at the host
%institution (for instance, Softbank Nao and Pepper). This user-centered approach
%will be championed by the post-doc recruited on the project on WP4 and WP5, who will
%have to have a strong expertise in user-centered design.
%
%It is also important to note that, while preparing this bid, initial discussions
%have been held with all the partners involved with the experimental fieldwork
%(WeTheCurious science centre, Bristol's Children Hospital, the network of SEN
%schools): each of these institutions is enthusiastic about the project, already
%contributing ideas to integrate the robots in their daily routines, and
%ready to dedicate time and effort for its success.
%
%%
%%The PI, Pr. S√©verin Lemaignan, has been working for 12+
%%years in human-robot interaction, and over the last 6 years, specifically in the
%%field of child-robot interaction.  His profile is both highly technical, with
%%hundreds of hardware and software contributions to the worldwide robotic
%%community; and highly experimental, running dozens of studies and experiments
%%with children, including long-term ones, in multiple schools and in healthcare
%%environments. He is in a unique position to deliver both a scientific
%%breakthrough on social situation assessment for robots, and a high-impact
%%societal change.
%
%%\begin{table}[!htbp]
%%\caption{Identified risks and proposed mitigations}
%%\centering
%%\begin{tabular}{@{}llll@{}}
%%\toprule
%%    \textbf{Task} & \textbf{Description of risk}                 & \textbf{Level} & \textbf{Proposed risk mitigation measures} \\ \midrule
%%    T1.1          & Visitors' interactions with robot too short to create
%%    meaningful social interaction                            & High
%%    & The 'public-in-the-loop' machine learning methodology is high risk/high
%%    gain; machine                            \\
%%                  &                              &                &                            \\
%%                  &                              &                &                            \\
%%                  &                              &                &                            \\ \bottomrule
%%\end{tabular}
%%\end{table}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%\newpage

